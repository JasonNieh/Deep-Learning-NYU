{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "azOQDGfMLFN8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import math\n",
    "import torchvision\n",
    "from torchvision import transforms as transforms\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from math import cos,pi\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nnaef49GOhPH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Vb_Gr3w9vzx8"
   },
   "outputs": [],
   "source": [
    "# torch.manual_seed(17)\n",
    "\n",
    "# class HaS(object): \n",
    "# #     def __init__(self):\n",
    "        \n",
    "#     def __call__(self, img):\n",
    "#         # get width and height of the image\n",
    "#         img_= np.array(img).copy()\n",
    "#         s = img_.shape\n",
    "#         wd = s[0]\n",
    "#         ht = s[1]\n",
    "\n",
    "#         # possible grid size, 0 means no hiding\n",
    "#         grid_size=3\n",
    "\n",
    "#         # hiding probability\n",
    "#         hide_prob = 0.1\n",
    " \n",
    "#         # randomly choose one grid size\n",
    "# #         grid_size= grid_sizes[random.randint(0,len(grid_sizes)-1)]\n",
    "\n",
    "#         # hide the patches\n",
    "#         if(grid_size>0):\n",
    "#              for x in range(0,wd,grid_size):\n",
    "#                  for y in range(0,ht,grid_size):\n",
    "#                      x_end = min(wd, x+grid_size)  \n",
    "#                      y_end = min(ht, y+grid_size)\n",
    "#                      if(random.random() <=  hide_prob):\n",
    "#                            img_[x:x_end,y:y_end,:]=0\n",
    "\n",
    "#         return img_\n",
    "    \n",
    "# torch.manual_seed(17)\n",
    "\n",
    "        \n",
    "# class HideEdge(object): \n",
    "#     def __init__(self,hide_size):\n",
    "#         self.hide_size=hide_size\n",
    "        \n",
    "#     def __call__(self, img):\n",
    "#         # get width and height of the image\n",
    "#         img_= np.array(img).copy()\n",
    "#         s = img_.shape\n",
    "#         wd = s[0]\n",
    "#         ht = s[1]\n",
    "\n",
    "#         hide_size=self.hide_size\n",
    "        \n",
    "# #         img_[:,:,:] = img()\n",
    "   \n",
    "#         x_end = wd - hide_size \n",
    "#         y_end = ht - hide_size\n",
    "\n",
    "#         img_[x_end:,y_end:,:]=0\n",
    "# #         img_[x_end:,:hide_size,:]=0\n",
    "# #         img_[:hide_size,y_end:,:]=0\n",
    "#         img_[:hide_size,:hide_size,:]=0\n",
    "# #         img_[x_end:,:,:]=0\n",
    "# #         img_[:,y_end:,:]=0\n",
    "# #         img_[:hide_size,:,:]=0\n",
    "# #         img_[:,:hide_size,:]=0\n",
    "# #         print(img_[x_end,y_end,:])\n",
    "# #         print(img_[hide_size,hide_size,:])\n",
    "# #         print(x_end,y_end,hide_size)\n",
    "        \n",
    "# #         mean = img_[hide_size:x_end-1,hide_size:y_end,:].mean()\n",
    "# #         std = img_[hide_size:x_end-1,hide_size:y_end,:].std()\n",
    "# #         print(mean, std)\n",
    "# #         img_[hide_size:x_end-1,hide_size:y_end,:] = (img_[hide_size:x_end-1,hide_size:y_end,:] - mean) / std\n",
    "# #         print(img_[hide_size:x_end-1,hide_size:y_end,:])\n",
    "        \n",
    "#         return img_\n",
    "\n",
    "   \n",
    "# class Hide_after_Norm(object): \n",
    "#     def __init__(self,hide_size):\n",
    "#         self.hide_size=hide_size\n",
    "        \n",
    "#     def __call__(self, img_):\n",
    "#         # get width and height of the image\n",
    "# #         img_= np.array(img).copy()\n",
    "#         s = img_.shape\n",
    "#         wd = s[1]\n",
    "#         ht = s[2]\n",
    "\n",
    "#         hide_size=self.hide_size\n",
    "        \n",
    "# #         img_[:,:,:] = img()\n",
    "   \n",
    "#         x_end = wd - hide_size \n",
    "#         y_end = ht - hide_size\n",
    "        \n",
    "#         x_end = wd - hide_size \n",
    "#         y_end = ht - hide_size\n",
    "\n",
    "#         img_[:,x_end:,y_end:]=0\n",
    "# #         img_[x_end:,:hide_size,:]=0\n",
    "# #         img_[:hide_size,y_end:,:]=0\n",
    "#         img_[:,:hide_size,:hide_size]=0\n",
    "# #         print(img_[x_end,y_end,:])\n",
    "# #         print(img_[hide_size,hide_size,:])\n",
    "# #         print(x_end,y_end,hide_size)\n",
    "        \n",
    "# #         mean = img_[hide_size:x_end-1,hide_size:y_end,:].mean()\n",
    "# #         std = img_[hide_size:x_end-1,hide_size:y_end,:].std()\n",
    "# #         print(mean, std)\n",
    "# #         img_[hide_size:x_end-1,hide_size:y_end,:] = (img_[hide_size:x_end-1,hide_size:y_end,:] - mean) / std\n",
    "# #         print(img_[hide_size:x_end-1,hide_size:y_end,:])\n",
    "        \n",
    "#         return img_\n",
    "    \n",
    "    \n",
    "\n",
    "# # torch.cuda.manual_seed(17) # for GPU\n",
    "# aug_train = transforms.Compose([\n",
    "#     transforms.RandomHorizontalFlip(), # 水平翻转\n",
    "# #     torchvision.transforms.CenterCrop(26),\n",
    "# #     HideEdge(),\n",
    "#     torchvision.transforms.RandomRotation(15),\n",
    "# #     torchvision.transforms.CenterCrop(28),\n",
    "#     # transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5), # color aug\n",
    "# #     transforms.RandomCrop(32, padding=4), # 裁剪\n",
    "#     # transforms.RandomResizedCrop((32,32),scale=(0.1,1),ratio=(0.5,2))\n",
    "# #     hide_patch(),\n",
    "# #     HaS(),\n",
    "# #     HideEdge(2),\n",
    "#     transforms.ToTensor(),\n",
    "# #     Norm(2),\n",
    "#     transforms.Normalize((0.4649, 0.4553, 0.4214), (0.2271, 0.2234, 0.2208)),# normalization\n",
    "#     Hide_after_Norm(2)\n",
    "#     ])\n",
    "\n",
    "# aug_test = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.4649, 0.4553, 0.4214), (0.2271, 0.2234, 0.2208)), # normalization\n",
    "#     Hide_after_Norm(2)\n",
    "#     ])\n",
    "\n",
    "# trainingdata = torchvision.datasets.CIFAR10('./CIFAR10',train=True,download=True,transform=aug_train)\n",
    "# # testdata = torchvision.datasets.CIFAR10('./CIFAR10',train=False,download=True,transform=transforms.ToTensor())\n",
    "# # print(len(trainingdata),len(testdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(17)\n",
    "torch.cuda.manual_seed_all(17)\n",
    "\n",
    "aug_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32,padding=4,padding_mode='reflect'),\n",
    "    transforms.RandomHorizontalFlip(), # 水平翻转\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4244, 0.4146, 0.3836), (0.2539, 0.2491, 0.2420)) # normalization\n",
    "    ])\n",
    "\n",
    "aug_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4244, 0.4146, 0.3836), (0.2539, 0.2491, 0.2420)) # normalization\n",
    "    ])\n",
    "\n",
    "trainingdata = torchvision.datasets.CIFAR10('./CIFAR10',train=True,download=True,transform=aug_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103,
     "referenced_widgets": [
      "3983cfe844f847899487b2745a203a9b",
      "1121692d91114d58b3db79e41b0a24c9",
      "8148f05b38d94ec19351ac920b70acba",
      "af7b660f147d4d1d8796d3419673c9b7",
      "0130588af6254c76a2c8c382288cfcea",
      "fb3e769eb8324af3b77041879c9b54cf",
      "accf52fe298b4716af9d9c351e7326dc",
      "1d40f4bf0c0e48709cdec5f80069ed2f",
      "321ea7fa0b4d4c9dab9ed5231954e54c",
      "a66b114e4595419f84ac44a676a1ca61",
      "4aa5668c8eaa49f88148d499240b6ea5"
     ]
    },
    "id": "1lqsbqYCMja7",
    "outputId": "3b4aa629-06a0-480a-afec-dcdb971e4bf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def get_mean_and_std(dataset):\n",
    "  '''Compute the mean and std value of dataset.'''\n",
    "  dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "  mean = torch.zeros(3)\n",
    "  std = torch.zeros(3)\n",
    "  print('==> Computing mean and std..')\n",
    "  for inputs, targets in dataloader:\n",
    "      for i in range(3):\n",
    "          mean[i] += inputs[:,i,:,:].mean()\n",
    "          std[i] += inputs[:,i,:,:].std()\n",
    "  mean.div_(len(dataset))\n",
    "  std.div_(len(dataset))\n",
    "  return mean, std\n",
    "\n",
    "def load_data(is_train,aug,batch_size):\n",
    "  dataset = torchvision.datasets.CIFAR10('./CIFAR10',train=is_train,download=True,transform=aug)\n",
    "#   mean, std = get_mean_and_std(dataset)\n",
    "#   print(mean, std)\n",
    "  dataloader = torch.utils.data.DataLoader(dataset,batch_size=batch_size,shuffle=is_train)\n",
    "  return dataloader\n",
    "\n",
    "batch_size = 256 # param\n",
    "trainDataLoader = load_data(is_train=True,aug=aug_train,batch_size=batch_size)\n",
    "testDataLoader = load_data(is_train=False,aug=aug_test,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-TD4UKtgzXbh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32]) 6\n",
      "[[[-0.9147032  -0.38956204  0.0274618  ...  0.01201648  0.18191507\n",
      "    0.15102442]\n",
      "  [-0.32778072 -0.03431951  0.10468844 ... -0.21966343  0.1973604\n",
      "    0.2591417 ]\n",
      "  [-0.09610081  0.29003236  0.24369638 ... -0.6366873  -0.51312464\n",
      "   -0.32778072]\n",
      "  ...\n",
      "  [ 1.0931895   1.2321974   1.448432   ...  0.22825105  0.66072035\n",
      "    1.6646665 ]\n",
      "  [ 1.2013068   1.3248694   1.3866507  ... -0.38956204 -0.17332745\n",
      "    1.1704161 ]\n",
      "  [ 1.3866507   1.2785335   1.1549708  ... -0.8529219  -0.8065859\n",
      "    0.7997283 ]]\n",
      "\n",
      " [[-1.2393323  -0.8772444  -0.53089947 ... -0.5151565  -0.3419841\n",
      "   -0.3419841 ]\n",
      "  [-0.8142726  -0.6725861  -0.5623854  ... -0.68832904 -0.32624117\n",
      "   -0.29475525]\n",
      "  [-0.6411001  -0.42069885 -0.45218474 ... -1.003188   -0.9244732\n",
      "   -0.76704377]\n",
      "  ...\n",
      "  [ 0.57110703  0.6813077   0.9804237  ... -0.21604052  0.19327614\n",
      "    1.2323109 ]\n",
      "  [ 0.6025929   0.74427944  0.8229942  ... -0.83001554 -0.68832904\n",
      "    0.6655647 ]\n",
      "  [ 0.87022305  0.8072512   0.6340788  ... -1.1291317  -1.1763605\n",
      "    0.42942047]]\n",
      "\n",
      " [[-1.4554853  -1.212413   -0.92072594 ... -0.9045211  -0.7748825\n",
      "   -0.7748825 ]\n",
      "  [-1.1800033  -1.1313888  -1.0503646  ... -1.017955   -0.80729216\n",
      "   -0.7748825 ]\n",
      "  [-1.0341598  -0.9531356  -1.0017501  ... -1.1800033  -1.212413\n",
      "   -1.1475936 ]\n",
      "  ...\n",
      "  [-0.17530379 -0.49940035 -0.4831955  ... -0.4183762  -0.22391827\n",
      "    0.6835522 ]\n",
      "  [-1.0989791  -1.1800033  -1.0341598  ... -1.0341598  -1.0341598\n",
      "   -0.06186999]\n",
      "  [-1.1637985  -1.1475936  -1.0341598  ... -1.2610275  -1.4716902\n",
      "   -0.45078588]]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "image,label = trainingdata[0]\n",
    "image_= np.array(image).copy()\n",
    "print(image.shape, label)\n",
    "print(image_)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.imshow(image.numpy().transpose(1,2,0))\n",
    "plt.title(str(classes[label]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jo4vcA1BwajW"
   },
   "outputs": [],
   "source": [
    "# trainDataLoader = torch.utils.data.DataLoader(trainingdata,batch_size=batch_size,shuffle=True)\n",
    "# testDataLoader = torch.utils.data.DataLoader(testdata,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "# images, labels = iter(trainDataLoader).next()\n",
    "# plt.figure(figsize=(17,8))\n",
    "# for index in np.arange(0,5):\n",
    "#   plt.subplot(1,5,index+1)\n",
    "#   plt.imshow(images[index].numpy().transpose(1,2,0))\n",
    "#   plt.title(str(classes[labels[index]]))\n",
    "\n",
    "def get_mean_and_std(dataset):\n",
    "  '''Compute the mean and std value of dataset.'''\n",
    "  dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "  mean = torch.zeros(3)\n",
    "  std = torch.zeros(3)\n",
    "  print('==> Computing mean and std..')\n",
    "  for inputs, targets in dataloader:\n",
    "      for i in range(3):\n",
    "          mean[i] += inputs[:,i,:,:].mean()\n",
    "          std[i] += inputs[:,i,:,:].std()\n",
    "  mean.div_(len(dataset))\n",
    "  std.div_(len(dataset))\n",
    "  return mean, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YDBTjSf2jDNm"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, in_planes, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = in_planes\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, self.in_planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_planes)\n",
    "        self.layer1 = self._make_layer(block, in_planes, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, in_planes*2, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, in_planes*4, num_blocks[2], stride=2)\n",
    "#         self.layer4 = self._make_layer(block, in_planes*8, num_blocks[3], stride=2)\n",
    "#         self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "#         self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "#         self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "#         self.layer4 = self._make_layer(block, self.in_planes*8, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(1280, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "#         print(out.shape)\n",
    "        out = self.layer1(out)\n",
    "#         print(out.shape)\n",
    "        out = self.layer2(out)\n",
    "#         print(out.shape)\n",
    "        out = self.layer3(out)\n",
    "#         print(out.shape)\n",
    "#         out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "#         print(out.shape)\n",
    "        out = out.view(out.size(0), -1)\n",
    "#         print(out.shape)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight,mode='fan_out',nonlinearity='relu') # weight initialization\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias,0)\n",
    "            elif isinstance(m,nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight,1)\n",
    "                nn.init.constant_(m.bias,0)\n",
    "            elif isinstance(m,nn.Linear):\n",
    "                nn.init.normal_(m.weight,std=1e-3)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias,0)    \n",
    "\n",
    "def project1_model():\n",
    "#     return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "#     return ResNet(BasicBlock, [2, 2, 2])\n",
    "    return ResNet(80, BasicBlock, [3, 3, 2])\n",
    "\n",
    "# model1 = nn.Sequential(project1_model(), nn.AdaptiveAvgPool2d((1,1)), nn.Flatten(), nn.Linear(512, 10)).cuda()\n",
    "model1 = project1_model().cuda()\n",
    "model1.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FDnI9zbyLK6B",
    "outputId": "4a1b7b4e-5d52-42d6-8563-500023c3acae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4923930\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    # torch.numel() returns number of elements in a tensor\n",
    "\n",
    "print(count_parameters(model1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, current_epoch,max_epoch,lr_min=0,lr_max=0.1,warmup=True):\n",
    "    warmup_epoch = 10 if warmup else 0\n",
    "    if current_epoch < warmup_epoch:\n",
    "        lr = lr_max * current_epoch / warmup_epoch\n",
    "    else:\n",
    "        lr = lr_min + (lr_max-lr_min)*(1 + cos(pi * (current_epoch - warmup_epoch) / (max_epoch - warmup_epoch))) / 2\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HbLtFYydjoIx",
    "outputId": "0321ff7b-c60c-4099-f5fb-4a9536f10244"
   },
   "outputs": [],
   "source": [
    "# X = torch.rand(size=(1, 3, 32, 32)).cuda()\n",
    "# for layer in model1:\n",
    "#   X = layer(X)\n",
    "#   print(layer.__class__.__name__,'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "id": "j5xklXYe6gRe",
    "outputId": "b846ff14-cc67-4bdc-d6bb-f727fa580bcc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read model from checkpoint\n",
      "Restart from epoch 1860\n",
      "Epoch 1861, Train loss 2.0812114671642234e-06, Test loss 0.44228804148733614, Train accuracy 100.0, Test accuracy 94.599609375, Cost 61.099342823028564 s\n",
      "Epoch 1862, Train loss 9.015377996371138e-06, Test loss 0.442588172480464, Train accuracy 100.0, Test accuracy 94.560546875, Cost 61.20502972602844 s\n",
      "Epoch 1863, Train loss 3.1168270294950608e-06, Test loss 0.44271974712610246, Train accuracy 100.0, Test accuracy 94.580078125, Cost 61.30597925186157 s\n",
      "Epoch 1864, Train loss 1.4894234675622463e-06, Test loss 0.44273002706468106, Train accuracy 100.0, Test accuracy 94.580078125, Cost 61.46337342262268 s\n",
      "Epoch 1865, Train loss 1.3273172771228883e-06, Test loss 0.4427560370415449, Train accuracy 100.0, Test accuracy 94.580078125, Cost 61.47428369522095 s\n",
      "Model saved in epoch 1865\n",
      "Epoch 1866, Train loss 1.2808736773903998e-06, Test loss 0.4427588518708944, Train accuracy 100.0, Test accuracy 94.580078125, Cost 67.58884143829346 s\n",
      "Epoch 1867, Train loss 1.742703294750823e-06, Test loss 0.4426891081035137, Train accuracy 100.0, Test accuracy 94.580078125, Cost 82.77144646644592 s\n",
      "Epoch 1868, Train loss 1.85655276128535e-06, Test loss 0.44260766841471194, Train accuracy 100.0, Test accuracy 94.58984375, Cost 83.11540269851685 s\n",
      "Epoch 1869, Train loss 2.0754374147806087e-06, Test loss 0.44261358343064783, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.59003639221191 s\n",
      "Epoch 1870, Train loss 1.4868545149288604e-06, Test loss 0.44265800788998605, Train accuracy 100.0, Test accuracy 94.580078125, Cost 82.67134761810303 s\n",
      "Model saved in epoch 1870\n",
      "Epoch 1871, Train loss 1.6765178835674379e-06, Test loss 0.4426691021770239, Train accuracy 100.0, Test accuracy 94.58984375, Cost 82.66122198104858 s\n",
      "Epoch 1872, Train loss 1.664558580400615e-06, Test loss 0.4426502365618944, Train accuracy 100.0, Test accuracy 94.58984375, Cost 82.62109518051147 s\n",
      "Epoch 1873, Train loss 1.4576392651992258e-06, Test loss 0.4426395554095507, Train accuracy 100.0, Test accuracy 94.580078125, Cost 82.24696278572083 s\n",
      "Epoch 1874, Train loss 1.575480892068488e-06, Test loss 0.4426951192319393, Train accuracy 100.0, Test accuracy 94.580078125, Cost 82.61656928062439 s\n",
      "Epoch 1875, Train loss 4.106367272048694e-06, Test loss 0.4421011213213205, Train accuracy 100.0, Test accuracy 94.560546875, Cost 82.55603623390198 s\n",
      "Model saved in epoch 1875\n",
      "Epoch 1876, Train loss 1.548577059926796e-06, Test loss 0.4421204660087824, Train accuracy 100.0, Test accuracy 94.560546875, Cost 82.6750373840332 s\n",
      "Epoch 1877, Train loss 1.7008418586464571e-06, Test loss 0.4421136572957039, Train accuracy 100.0, Test accuracy 94.560546875, Cost 82.6297698020935 s\n",
      "Epoch 1878, Train loss 1.1544384487091212e-06, Test loss 0.4421550750732422, Train accuracy 100.0, Test accuracy 94.560546875, Cost 82.33773446083069 s\n",
      "Epoch 1879, Train loss 1.5286694948428579e-06, Test loss 0.4421324644237757, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.70691180229187 s\n",
      "Epoch 1880, Train loss 1.7555280687373734e-06, Test loss 0.4420758783817291, Train accuracy 100.0, Test accuracy 94.580078125, Cost 82.84567880630493 s\n",
      "Model saved in epoch 1880\n",
      "Epoch 1881, Train loss 1.321938717520655e-06, Test loss 0.44208964742720125, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.8629858493805 s\n",
      "Epoch 1882, Train loss 2.269464125850319e-06, Test loss 0.44206903092563155, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.43628430366516 s\n",
      "Epoch 1883, Train loss 1.2413487105172758e-06, Test loss 0.4421356346458197, Train accuracy 100.0, Test accuracy 94.580078125, Cost 82.83002424240112 s\n",
      "Epoch 1884, Train loss 1.0986108510441806e-06, Test loss 0.4421413969248533, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.45965886116028 s\n",
      "Epoch 1885, Train loss 1.5879679516883627e-06, Test loss 0.4421685118228197, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.70221543312073 s\n",
      "Model saved in epoch 1885\n",
      "Epoch 1886, Train loss 2.3327450615255285e-06, Test loss 0.4421471029520035, Train accuracy 100.0, Test accuracy 94.58984375, Cost 82.60664439201355 s\n",
      "Epoch 1887, Train loss 8.175691528041467e-07, Test loss 0.4421314250677824, Train accuracy 100.0, Test accuracy 94.58984375, Cost 82.6868793964386 s\n",
      "Epoch 1888, Train loss 1.5002750464176984e-06, Test loss 0.4421571370214224, Train accuracy 100.0, Test accuracy 94.58984375, Cost 82.83444118499756 s\n",
      "Epoch 1889, Train loss 1.0677420802348502e-06, Test loss 0.44216477423906325, Train accuracy 100.0, Test accuracy 94.580078125, Cost 82.45013928413391 s\n",
      "Epoch 1890, Train loss 1.4604143296586308e-06, Test loss 0.44221958220005037, Train accuracy 100.0, Test accuracy 94.58984375, Cost 82.94651341438293 s\n",
      "Model saved in epoch 1890\n",
      "Epoch 1891, Train loss 8.947547687436865e-07, Test loss 0.4421874932944775, Train accuracy 100.0, Test accuracy 94.580078125, Cost 82.9332869052887 s\n",
      "Epoch 1892, Train loss 1.3994680955945992e-06, Test loss 0.4422021873295307, Train accuracy 100.0, Test accuracy 94.580078125, Cost 82.54548263549805 s\n",
      "Epoch 1893, Train loss 1.24322499661882e-06, Test loss 0.4421779502183199, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.52376866340637 s\n",
      "Epoch 1894, Train loss 9.82967797335181e-07, Test loss 0.44218861050903796, Train accuracy 100.0, Test accuracy 94.580078125, Cost 82.78130006790161 s\n",
      "Epoch 1895, Train loss 1.3909241970535603e-06, Test loss 0.4420945100486279, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.81051588058472 s\n",
      "Model saved in epoch 1895\n",
      "Epoch 1896, Train loss 1.896740747874701e-06, Test loss 0.4419660270214081, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.68526124954224 s\n",
      "Epoch 1897, Train loss 1.261269050760829e-06, Test loss 0.44196105003356934, Train accuracy 100.0, Test accuracy 94.5703125, Cost 83.16680955886841 s\n",
      "Epoch 1898, Train loss 1.1031752774683072e-06, Test loss 0.4419438973069191, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.88239312171936 s\n",
      "Epoch 1899, Train loss 1.5990481364059464e-06, Test loss 0.44207615293562413, Train accuracy 100.0, Test accuracy 94.55078125, Cost 82.98570084571838 s\n",
      "Epoch 1900, Train loss 1.0475792403156282e-06, Test loss 0.4420849345624447, Train accuracy 100.0, Test accuracy 94.55078125, Cost 82.6474015712738 s\n",
      "Model saved in epoch 1900\n",
      "Epoch 1901, Train loss 2.7981749320246494e-06, Test loss 0.4419914871454239, Train accuracy 100.0, Test accuracy 94.560546875, Cost 82.48697757720947 s\n",
      "Epoch 1902, Train loss 1.6088052690859881e-06, Test loss 0.4419534135609865, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.93580651283264 s\n",
      "Epoch 1903, Train loss 2.0100562996374232e-06, Test loss 0.44209958165884017, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.74429893493652 s\n",
      "Epoch 1904, Train loss 1.2028033725663513e-06, Test loss 0.4421220313757658, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.97378754615784 s\n",
      "Epoch 1905, Train loss 1.698910242101738e-06, Test loss 0.442145761102438, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.68899130821228 s\n",
      "Model saved in epoch 1905\n",
      "Epoch 1906, Train loss 1.4185778614369016e-06, Test loss 0.4420729953795671, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.772132396698 s\n",
      "Epoch 1907, Train loss 1.5345071569879216e-06, Test loss 0.44200555607676506, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.54136490821838 s\n",
      "Epoch 1908, Train loss 1.2744030741535075e-06, Test loss 0.4420689079910517, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.46389770507812 s\n",
      "Epoch 1909, Train loss 1.1319444295085413e-06, Test loss 0.442091665789485, Train accuracy 100.0, Test accuracy 94.560546875, Cost 82.78495764732361 s\n",
      "Epoch 1910, Train loss 1.4174251874262394e-06, Test loss 0.4421239044517279, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.73889446258545 s\n",
      "Model saved in epoch 1910\n",
      "Epoch 1911, Train loss 1.6502199374790925e-06, Test loss 0.44209223091602323, Train accuracy 100.0, Test accuracy 94.560546875, Cost 82.83714962005615 s\n",
      "Epoch 1912, Train loss 1.6865435922660476e-06, Test loss 0.44212774448096753, Train accuracy 100.0, Test accuracy 94.580078125, Cost 82.80493021011353 s\n",
      "Epoch 1913, Train loss 1.0020919813674814e-06, Test loss 0.44213650152087214, Train accuracy 100.0, Test accuracy 94.580078125, Cost 82.8249888420105 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1914, Train loss 1.377811521600171e-06, Test loss 0.4421252977102995, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.5269136428833 s\n",
      "Epoch 1915, Train loss 1.3193201192119434e-06, Test loss 0.4421601820737123, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.51550602912903 s\n",
      "Model saved in epoch 1915\n",
      "Epoch 1916, Train loss 1.226258361805169e-06, Test loss 0.4421677220612764, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.78033661842346 s\n",
      "Epoch 1917, Train loss 1.049152235047829e-06, Test loss 0.4422636043280363, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.3441789150238 s\n",
      "Epoch 1918, Train loss 1.210734964720391e-06, Test loss 0.44221354834735394, Train accuracy 100.0, Test accuracy 94.580078125, Cost 83.05810308456421 s\n",
      "Epoch 1919, Train loss 1.1068310430361253e-06, Test loss 0.4422152727842331, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.52970266342163 s\n",
      "Epoch 1920, Train loss 1.3467875318012516e-06, Test loss 0.4422637462615967, Train accuracy 100.0, Test accuracy 94.58984375, Cost 82.78470396995544 s\n",
      "Model saved in epoch 1920\n",
      "Epoch 1921, Train loss 1.305001842398314e-06, Test loss 0.44216994754970074, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.50562000274658 s\n",
      "Epoch 1922, Train loss 8.5641276307348e-07, Test loss 0.4421672474592924, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.83461308479309 s\n",
      "Epoch 1923, Train loss 1.238697948503758e-06, Test loss 0.4421970274299383, Train accuracy 100.0, Test accuracy 94.560546875, Cost 82.32117581367493 s\n",
      "Epoch 1924, Train loss 1.2802269399331608e-06, Test loss 0.44217676632106306, Train accuracy 100.0, Test accuracy 94.580078125, Cost 82.69444155693054 s\n",
      "Epoch 1925, Train loss 1.2307063997023618e-06, Test loss 0.44208385460078714, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.73338294029236 s\n",
      "Model saved in epoch 1925\n",
      "Epoch 1926, Train loss 1.1984865784106569e-06, Test loss 0.44209875278174876, Train accuracy 100.0, Test accuracy 94.5703125, Cost 83.00548768043518 s\n",
      "Epoch 1927, Train loss 1.0627187342734311e-06, Test loss 0.44210900701582434, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.63483023643494 s\n",
      "Epoch 1928, Train loss 3.0931074532191557e-06, Test loss 0.44225155636668206, Train accuracy 100.0, Test accuracy 94.560546875, Cost 82.57601308822632 s\n",
      "Epoch 1929, Train loss 1.2607103790850145e-06, Test loss 0.44229187816381454, Train accuracy 100.0, Test accuracy 94.560546875, Cost 82.6951847076416 s\n",
      "Epoch 1930, Train loss 1.1742773307202248e-06, Test loss 0.44231435693800447, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.45532202720642 s\n",
      "Model saved in epoch 1930\n",
      "Epoch 1931, Train loss 9.648889886809788e-07, Test loss 0.4423263568431139, Train accuracy 100.0, Test accuracy 94.580078125, Cost 82.58871340751648 s\n",
      "Epoch 1932, Train loss 1.1591440115238043e-06, Test loss 0.44231906197965143, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.71024942398071 s\n",
      "Epoch 1933, Train loss 1.2374033545841644e-06, Test loss 0.4423598352819681, Train accuracy 100.0, Test accuracy 94.5703125, Cost 82.69975590705872 s\n",
      "Epoch 1934, Train loss 1.6978765560488846e-06, Test loss 0.4422756500542164, Train accuracy 100.0, Test accuracy 94.580078125, Cost 82.70556545257568 s\n",
      "Epoch 1935, Train loss 1.7766220916467218e-06, Test loss 0.4421738777309656, Train accuracy 100.0, Test accuracy 94.580078125, Cost 82.42637825012207 s\n",
      "Model saved in epoch 1935\n",
      "Epoch 1936, Train loss 1.681329701867257e-06, Test loss 0.44212518334388734, Train accuracy 100.0, Test accuracy 94.58984375, Cost 82.78301072120667 s\n",
      "Epoch 1937, Train loss 1.4088443924038654e-06, Test loss 0.442206446826458, Train accuracy 100.0, Test accuracy 94.58984375, Cost 82.208744764328 s\n",
      "Epoch 1938, Train loss 1.0934898813097504e-06, Test loss 0.4422321505844593, Train accuracy 100.0, Test accuracy 94.58984375, Cost 82.77814030647278 s\n",
      "Epoch 1939, Train loss 1.1181508844812787e-06, Test loss 0.44223932549357414, Train accuracy 100.0, Test accuracy 94.58984375, Cost 82.80224895477295 s\n",
      "Epoch 1940, Train loss 1.130823416366817e-06, Test loss 0.44215553291141985, Train accuracy 100.0, Test accuracy 94.58984375, Cost 82.65800619125366 s\n",
      "Model saved in epoch 1940\n",
      "Epoch 1941, Train loss 1.2247282987946379e-06, Test loss 0.44215860962867737, Train accuracy 100.0, Test accuracy 94.58984375, Cost 82.52104353904724 s\n",
      "Epoch 1942, Train loss 1.1854130468964914e-06, Test loss 0.44218374192714693, Train accuracy 100.0, Test accuracy 94.58984375, Cost 82.58889532089233 s\n",
      "Epoch 1943, Train loss 1.7671844564382431e-06, Test loss 0.44221777841448784, Train accuracy 100.0, Test accuracy 94.58984375, Cost 82.60784101486206 s\n",
      "Epoch 1944, Train loss 2.1591402271398673e-06, Test loss 0.44195321910083296, Train accuracy 100.0, Test accuracy 94.58984375, Cost 82.5926821231842 s\n",
      "Epoch 1945, Train loss 1.1382463980066076e-06, Test loss 0.4419534210115671, Train accuracy 100.0, Test accuracy 94.580078125, Cost 82.81650900840759 s\n",
      "Model saved in epoch 1945\n",
      "Epoch 1946, Train loss 1.6357490615711046e-06, Test loss 0.4419039096683264, Train accuracy 100.0, Test accuracy 94.58984375, Cost 82.2965943813324 s\n",
      "Epoch 1947, Train loss 1.7766253728179748e-06, Test loss 0.44188767299056053, Train accuracy 100.0, Test accuracy 94.58984375, Cost 82.49647498130798 s\n",
      "Epoch 1948, Train loss 1.1836794551597749e-06, Test loss 0.4418865889310837, Train accuracy 100.0, Test accuracy 94.599609375, Cost 82.62791776657104 s\n",
      "Epoch 1949, Train loss 1.1608114991774293e-06, Test loss 0.4419133342802525, Train accuracy 100.0, Test accuracy 94.599609375, Cost 98.49970555305481 s\n",
      "Epoch 1950, Train loss 6.0259668990440295e-06, Test loss 0.44209987074136736, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.3590190410614 s\n",
      "Model saved in epoch 1950\n",
      "Epoch 1951, Train loss 1.395326872411631e-06, Test loss 0.4432201232761145, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.25928664207458 s\n",
      "Epoch 1952, Train loss 1.076626884668973e-06, Test loss 0.4431640774011612, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.02195763587952 s\n",
      "Epoch 1953, Train loss 1.7594003668114026e-06, Test loss 0.44312664568424226, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.46014380455017 s\n",
      "Epoch 1954, Train loss 2.857822622049944e-06, Test loss 0.44322383031249046, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.53033971786499 s\n",
      "Epoch 1955, Train loss 4.404449491323783e-06, Test loss 0.4423729073256254, Train accuracy 100.0, Test accuracy 94.609375, Cost 127.62352919578552 s\n",
      "Model saved in epoch 1955\n",
      "Epoch 1956, Train loss 1.6054094563302807e-06, Test loss 0.4425259493291378, Train accuracy 100.0, Test accuracy 94.619140625, Cost 126.78601169586182 s\n",
      "Epoch 1957, Train loss 1.7904337069448876e-06, Test loss 0.4426056742668152, Train accuracy 100.0, Test accuracy 94.609375, Cost 127.59755277633667 s\n",
      "Epoch 1958, Train loss 1.9464395405024697e-06, Test loss 0.44266664050519466, Train accuracy 100.0, Test accuracy 94.609375, Cost 127.32619261741638 s\n",
      "Epoch 1959, Train loss 1.8796487001338412e-06, Test loss 0.4425583690404892, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.15325546264648 s\n",
      "Epoch 1960, Train loss 1.010819686193905e-06, Test loss 0.4425858072936535, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.10596799850464 s\n",
      "Model saved in epoch 1960\n",
      "Epoch 1961, Train loss 1.8668943910057289e-06, Test loss 0.442503697425127, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.22813367843628 s\n",
      "Epoch 1962, Train loss 1.7908902893177044e-06, Test loss 0.4426214177161455, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.03214383125305 s\n",
      "Epoch 1963, Train loss 1.029677312303079e-06, Test loss 0.4425962768495083, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.21473407745361 s\n",
      "Epoch 1964, Train loss 1.0560455530730645e-06, Test loss 0.44258275628089905, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.58370161056519 s\n",
      "Epoch 1965, Train loss 2.9747381638511645e-06, Test loss 0.4425047133117914, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.36605453491211 s\n",
      "Model saved in epoch 1965\n",
      "Epoch 1966, Train loss 1.50047672089527e-06, Test loss 0.4425195846706629, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.53322792053223 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1967, Train loss 1.3401964402284974e-06, Test loss 0.44254229255020616, Train accuracy 100.0, Test accuracy 94.619140625, Cost 126.73389601707458 s\n",
      "Epoch 1968, Train loss 1.5215669887222053e-06, Test loss 0.44243624396622183, Train accuracy 100.0, Test accuracy 94.619140625, Cost 127.4806261062622 s\n",
      "Epoch 1969, Train loss 1.1747804645133564e-06, Test loss 0.442513570189476, Train accuracy 100.0, Test accuracy 94.619140625, Cost 126.36220622062683 s\n",
      "Epoch 1970, Train loss 1.916287769291155e-06, Test loss 0.4424312174320221, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.51894354820251 s\n",
      "Model saved in epoch 1970\n",
      "Epoch 1971, Train loss 2.467623017159732e-06, Test loss 0.44269776791334153, Train accuracy 100.0, Test accuracy 94.580078125, Cost 126.99126291275024 s\n",
      "Epoch 1972, Train loss 1.3618060587403396e-06, Test loss 0.44270134828984736, Train accuracy 100.0, Test accuracy 94.5703125, Cost 127.23377513885498 s\n",
      "Epoch 1973, Train loss 1.288392252948459e-06, Test loss 0.4426864992827177, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.51756644248962 s\n",
      "Epoch 1974, Train loss 1.6907408227929031e-06, Test loss 0.4424117386341095, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.05625462532043 s\n",
      "Epoch 1975, Train loss 1.4901744420375953e-06, Test loss 0.44249511882662773, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.05038857460022 s\n",
      "Model saved in epoch 1975\n",
      "Epoch 1976, Train loss 1.1411464402084534e-06, Test loss 0.4424953266978264, Train accuracy 100.0, Test accuracy 94.599609375, Cost 126.63416361808777 s\n",
      "Epoch 1977, Train loss 1.2651742019020635e-06, Test loss 0.4425679866224527, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.169025182724 s\n",
      "Epoch 1978, Train loss 1.27916585194018e-06, Test loss 0.44261299930512904, Train accuracy 100.0, Test accuracy 94.599609375, Cost 126.90686869621277 s\n",
      "Epoch 1979, Train loss 2.8054491143695587e-06, Test loss 0.4428272806107998, Train accuracy 100.0, Test accuracy 94.599609375, Cost 126.94434714317322 s\n",
      "Epoch 1980, Train loss 2.3417216343258e-06, Test loss 0.44278838224709033, Train accuracy 100.0, Test accuracy 94.609375, Cost 126.84203481674194 s\n",
      "Model saved in epoch 1980\n",
      "Epoch 1981, Train loss 1.2947948619809802e-06, Test loss 0.4427637178450823, Train accuracy 100.0, Test accuracy 94.609375, Cost 127.55643630027771 s\n",
      "Epoch 1982, Train loss 1.2847010776918543e-06, Test loss 0.44282325729727745, Train accuracy 100.0, Test accuracy 94.609375, Cost 127.64057731628418 s\n",
      "Epoch 1983, Train loss 1.738374100142582e-06, Test loss 0.44280654974281786, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.6515998840332 s\n",
      "Epoch 1984, Train loss 1.609061514659501e-06, Test loss 0.4427885089069605, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.14298987388611 s\n",
      "Epoch 1985, Train loss 1.6968398660044177e-06, Test loss 0.44274442456662655, Train accuracy 100.0, Test accuracy 94.599609375, Cost 126.78750014305115 s\n",
      "Model saved in epoch 1985\n",
      "Epoch 1986, Train loss 1.350466011467246e-06, Test loss 0.44276006482541563, Train accuracy 100.0, Test accuracy 94.609375, Cost 126.82848501205444 s\n",
      "Epoch 1987, Train loss 1.0498646069689185e-06, Test loss 0.44275871478021145, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.04622626304626 s\n",
      "Epoch 1988, Train loss 1.3852199977365872e-06, Test loss 0.4427836310118437, Train accuracy 100.0, Test accuracy 94.58984375, Cost 126.84033513069153 s\n",
      "Epoch 1989, Train loss 1.1633222319510429e-06, Test loss 0.4428132988512516, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.27391076087952 s\n",
      "Epoch 1990, Train loss 1.4752792305878465e-06, Test loss 0.4427374053746462, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.5370705127716 s\n",
      "Model saved in epoch 1990\n",
      "Epoch 1991, Train loss 1.301952710539748e-06, Test loss 0.442763302475214, Train accuracy 100.0, Test accuracy 94.619140625, Cost 127.50099587440491 s\n",
      "Epoch 1992, Train loss 1.2541701075271233e-06, Test loss 0.442777992784977, Train accuracy 100.0, Test accuracy 94.619140625, Cost 127.27358651161194 s\n",
      "Epoch 1993, Train loss 9.56844137211181e-07, Test loss 0.4428183492273092, Train accuracy 100.0, Test accuracy 94.62890625, Cost 127.58805894851685 s\n",
      "Epoch 1994, Train loss 1.138530042651794e-06, Test loss 0.44280772767961024, Train accuracy 100.0, Test accuracy 94.62890625, Cost 127.14882278442383 s\n",
      "Epoch 1995, Train loss 1.4359028562271665e-06, Test loss 0.4428771536797285, Train accuracy 100.0, Test accuracy 94.619140625, Cost 127.1505196094513 s\n",
      "Model saved in epoch 1995\n",
      "Epoch 1996, Train loss 1.5215801813528667e-06, Test loss 0.44290548861026763, Train accuracy 100.0, Test accuracy 94.619140625, Cost 127.1441638469696 s\n",
      "Epoch 1997, Train loss 1.4577975198965543e-06, Test loss 0.44289113245904443, Train accuracy 100.0, Test accuracy 94.619140625, Cost 127.61527752876282 s\n",
      "Epoch 1998, Train loss 1.2625937187461093e-06, Test loss 0.4427897181361914, Train accuracy 100.0, Test accuracy 94.619140625, Cost 127.6126823425293 s\n",
      "Epoch 1999, Train loss 1.2818166996040053e-06, Test loss 0.4427245300263166, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.16177868843079 s\n",
      "Epoch 2000, Train loss 1.3661651975680205e-06, Test loss 0.4427688460797071, Train accuracy 100.0, Test accuracy 94.609375, Cost 127.2631402015686 s\n",
      "Model saved in epoch 2000\n",
      "Epoch 2001, Train loss 1.3673998618481318e-06, Test loss 0.4427746865898371, Train accuracy 100.0, Test accuracy 94.609375, Cost 127.73908710479736 s\n",
      "Epoch 2002, Train loss 1.159019100576006e-06, Test loss 0.4427407566457987, Train accuracy 100.0, Test accuracy 94.619140625, Cost 127.44398522377014 s\n",
      "Epoch 2003, Train loss 1.386412326720432e-06, Test loss 0.442779765278101, Train accuracy 100.0, Test accuracy 94.62890625, Cost 127.29314970970154 s\n",
      "Epoch 2004, Train loss 2.497401137954317e-06, Test loss 0.4425950702279806, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.30070185661316 s\n",
      "Epoch 2005, Train loss 1.7984282037010893e-06, Test loss 0.4426580235362053, Train accuracy 100.0, Test accuracy 94.619140625, Cost 127.51118850708008 s\n",
      "Model saved in epoch 2005\n",
      "Epoch 2006, Train loss 1.3103102291171702e-06, Test loss 0.4426153715699911, Train accuracy 100.0, Test accuracy 94.609375, Cost 126.84855198860168 s\n",
      "Epoch 2007, Train loss 1.2560085017576207e-06, Test loss 0.4426185995340347, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.3244903087616 s\n",
      "Epoch 2008, Train loss 1.7806104548555782e-06, Test loss 0.44276669099926946, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.27083539962769 s\n",
      "Epoch 2009, Train loss 1.0267582979304598e-06, Test loss 0.44277021661400795, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.50365209579468 s\n",
      "Epoch 2010, Train loss 1.196572160018137e-06, Test loss 0.4427416671067476, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.41058874130249 s\n",
      "Model saved in epoch 2010\n",
      "Epoch 2011, Train loss 1.2153107712386636e-06, Test loss 0.44272210635244846, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.40813064575195 s\n",
      "Epoch 2012, Train loss 1.5417582871332262e-06, Test loss 0.4427534341812134, Train accuracy 100.0, Test accuracy 94.609375, Cost 127.05953931808472 s\n",
      "Epoch 2013, Train loss 1.3713497926523374e-06, Test loss 0.4428007513284683, Train accuracy 100.0, Test accuracy 94.609375, Cost 127.73038125038147 s\n",
      "Epoch 2014, Train loss 1.2188659301089057e-06, Test loss 0.44273032061755657, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.75793528556824 s\n",
      "Epoch 2015, Train loss 1.2945749504770073e-06, Test loss 0.44271078854799273, Train accuracy 100.0, Test accuracy 94.609375, Cost 127.42455005645752 s\n",
      "Model saved in epoch 2015\n",
      "Epoch 2016, Train loss 1.247160452462113e-06, Test loss 0.4426864981651306, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.09179759025574 s\n",
      "Epoch 2017, Train loss 1.2018619339635393e-06, Test loss 0.44268415197730066, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.69317650794983 s\n",
      "Epoch 2018, Train loss 1.2828404361825078e-06, Test loss 0.4426502637565136, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.67019772529602 s\n",
      "Epoch 2019, Train loss 1.8938869424559647e-06, Test loss 0.4426454443484545, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.61015224456787 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2020, Train loss 1.2604613344724176e-06, Test loss 0.44263956882059574, Train accuracy 100.0, Test accuracy 94.580078125, Cost 126.8394706249237 s\n",
      "Model saved in epoch 2020\n",
      "Epoch 2021, Train loss 1.341688526583195e-06, Test loss 0.44268174283206463, Train accuracy 100.0, Test accuracy 94.5703125, Cost 127.91428589820862 s\n",
      "Epoch 2022, Train loss 1.2521983033785923e-06, Test loss 0.4427076540887356, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.88338208198547 s\n",
      "Epoch 2023, Train loss 1.5340561373309257e-06, Test loss 0.442653875425458, Train accuracy 100.0, Test accuracy 94.580078125, Cost 126.5580472946167 s\n",
      "Epoch 2024, Train loss 1.8937751480545642e-06, Test loss 0.442512059956789, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.57271242141724 s\n",
      "Epoch 2025, Train loss 1.0087628138887735e-06, Test loss 0.44250273443758487, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.36474180221558 s\n",
      "Model saved in epoch 2025\n",
      "Epoch 2026, Train loss 1.428591982760461e-06, Test loss 0.4424392592161894, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.36083340644836 s\n",
      "Epoch 2027, Train loss 1.4422809078922103e-06, Test loss 0.44238448403775693, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.2107446193695 s\n",
      "Epoch 2028, Train loss 9.000142996587865e-07, Test loss 0.44237662814557555, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.15136790275574 s\n",
      "Epoch 2029, Train loss 1.5542822075101777e-06, Test loss 0.442394321039319, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.22584176063538 s\n",
      "Epoch 2030, Train loss 1.1322197240349455e-06, Test loss 0.44236926883459093, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.58899140357971 s\n",
      "Model saved in epoch 2030\n",
      "Epoch 2031, Train loss 1.5911834400106804e-06, Test loss 0.4423891041427851, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.45637321472168 s\n",
      "Epoch 2032, Train loss 1.2484035180490854e-06, Test loss 0.4424080058932304, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.5861701965332 s\n",
      "Epoch 2033, Train loss 1.0064158681036669e-06, Test loss 0.4423799328505993, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.42666816711426 s\n",
      "Epoch 2034, Train loss 9.082328631665357e-07, Test loss 0.44236260913312436, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.85403943061829 s\n",
      "Epoch 2035, Train loss 3.1342841997622383e-06, Test loss 0.4424904428422451, Train accuracy 100.0, Test accuracy 94.609375, Cost 127.2045226097107 s\n",
      "Model saved in epoch 2035\n",
      "Epoch 2036, Train loss 9.444763379711094e-06, Test loss 0.44174292013049127, Train accuracy 100.0, Test accuracy 94.521484375, Cost 126.87558221817017 s\n",
      "Epoch 2037, Train loss 1.5701787649501794e-06, Test loss 0.4417583405971527, Train accuracy 100.0, Test accuracy 94.521484375, Cost 126.7364993095398 s\n",
      "Epoch 2038, Train loss 1.4736758643498817e-06, Test loss 0.4418917089700699, Train accuracy 100.0, Test accuracy 94.51171875, Cost 127.18441939353943 s\n",
      "Epoch 2039, Train loss 2.170022430899293e-06, Test loss 0.4419719085097313, Train accuracy 100.0, Test accuracy 94.501953125, Cost 127.5113537311554 s\n",
      "Epoch 2040, Train loss 1.5866303613346916e-06, Test loss 0.4418371766805649, Train accuracy 100.0, Test accuracy 94.501953125, Cost 127.21515011787415 s\n",
      "Model saved in epoch 2040\n",
      "Epoch 2041, Train loss 2.298516271174688e-06, Test loss 0.44164135232567786, Train accuracy 100.0, Test accuracy 94.4921875, Cost 127.70624589920044 s\n",
      "Epoch 2042, Train loss 1.0897824546900786e-06, Test loss 0.441663920879364, Train accuracy 100.0, Test accuracy 94.4921875, Cost 127.12984538078308 s\n",
      "Epoch 2043, Train loss 1.6899637561641727e-06, Test loss 0.44168094396591184, Train accuracy 100.0, Test accuracy 94.4921875, Cost 128.49528551101685 s\n",
      "Epoch 2044, Train loss 1.3719443239155601e-06, Test loss 0.4417112607508898, Train accuracy 100.0, Test accuracy 94.501953125, Cost 127.23334622383118 s\n",
      "Epoch 2045, Train loss 9.76287821651471e-07, Test loss 0.44172608442604544, Train accuracy 100.0, Test accuracy 94.4921875, Cost 127.19998836517334 s\n",
      "Model saved in epoch 2045\n",
      "Epoch 2046, Train loss 1.1935362820156337e-06, Test loss 0.4417431756854057, Train accuracy 100.0, Test accuracy 94.4921875, Cost 127.49735569953918 s\n",
      "Epoch 2047, Train loss 1.0270044253391634e-06, Test loss 0.4417351331561804, Train accuracy 100.0, Test accuracy 94.4921875, Cost 127.94089841842651 s\n",
      "Epoch 2048, Train loss 1.4968696774467612e-06, Test loss 0.44176099114120004, Train accuracy 100.0, Test accuracy 94.4921875, Cost 127.39099168777466 s\n",
      "Epoch 2049, Train loss 1.4567237630534775e-06, Test loss 0.44183817617595195, Train accuracy 100.0, Test accuracy 94.501953125, Cost 127.35884165763855 s\n",
      "Epoch 2050, Train loss 1.088514976150721e-06, Test loss 0.4418212864547968, Train accuracy 100.0, Test accuracy 94.501953125, Cost 127.63044691085815 s\n",
      "Model saved in epoch 2050\n",
      "Epoch 2051, Train loss 1.3358820810003461e-06, Test loss 0.4418897371739149, Train accuracy 100.0, Test accuracy 94.521484375, Cost 126.97798180580139 s\n",
      "Epoch 2052, Train loss 1.4083022594349628e-06, Test loss 0.4419610772281885, Train accuracy 100.0, Test accuracy 94.51171875, Cost 127.35643601417542 s\n",
      "Epoch 2053, Train loss 8.698869094435601e-07, Test loss 0.4419680528342724, Train accuracy 100.0, Test accuracy 94.51171875, Cost 127.22327470779419 s\n",
      "Epoch 2054, Train loss 2.9861232441219974e-06, Test loss 0.44179320745170114, Train accuracy 100.0, Test accuracy 94.501953125, Cost 127.69331359863281 s\n",
      "Epoch 2055, Train loss 1.7771061222491688e-06, Test loss 0.44188756719231603, Train accuracy 100.0, Test accuracy 94.482421875, Cost 127.83053398132324 s\n",
      "Model saved in epoch 2055\n",
      "Epoch 2056, Train loss 1.5378867281505248e-06, Test loss 0.4419921305030584, Train accuracy 100.0, Test accuracy 94.501953125, Cost 127.42189335823059 s\n",
      "Epoch 2057, Train loss 1.4856589783702202e-06, Test loss 0.4419857650995255, Train accuracy 100.0, Test accuracy 94.51171875, Cost 127.47637915611267 s\n",
      "Epoch 2058, Train loss 1.5568617972492724e-06, Test loss 0.441937405243516, Train accuracy 100.0, Test accuracy 94.51171875, Cost 127.63296365737915 s\n",
      "Epoch 2059, Train loss 1.253368174145745e-06, Test loss 0.44184937737882135, Train accuracy 100.0, Test accuracy 94.501953125, Cost 127.31331300735474 s\n",
      "Epoch 2060, Train loss 1.1369277600549426e-06, Test loss 0.44179911240935327, Train accuracy 100.0, Test accuracy 94.501953125, Cost 127.57366919517517 s\n",
      "Model saved in epoch 2060\n",
      "Epoch 2061, Train loss 1.2263496983897231e-06, Test loss 0.44176629893481734, Train accuracy 100.0, Test accuracy 94.501953125, Cost 127.77500939369202 s\n",
      "Epoch 2062, Train loss 1.8550196850117104e-06, Test loss 0.44177481196820734, Train accuracy 100.0, Test accuracy 94.53125, Cost 127.70937824249268 s\n",
      "Epoch 2063, Train loss 1.5790916161782146e-06, Test loss 0.4418338142335415, Train accuracy 100.0, Test accuracy 94.53125, Cost 127.03364300727844 s\n",
      "Epoch 2064, Train loss 1.2757026610224656e-06, Test loss 0.4418543539941311, Train accuracy 100.0, Test accuracy 94.53125, Cost 127.3098497390747 s\n",
      "Epoch 2065, Train loss 1.7827988833688138e-06, Test loss 0.4418282262980938, Train accuracy 100.0, Test accuracy 94.51171875, Cost 127.58524203300476 s\n",
      "Model saved in epoch 2065\n",
      "Epoch 2066, Train loss 1.6399419594276833e-06, Test loss 0.4419972371309996, Train accuracy 100.0, Test accuracy 94.51171875, Cost 127.9988579750061 s\n",
      "Epoch 2067, Train loss 2.1238316338798082e-06, Test loss 0.4421534679830074, Train accuracy 100.0, Test accuracy 94.521484375, Cost 126.68721175193787 s\n",
      "Epoch 2068, Train loss 1.0142966551379582e-06, Test loss 0.44217599332332613, Train accuracy 100.0, Test accuracy 94.53125, Cost 127.30886840820312 s\n",
      "Epoch 2069, Train loss 1.3645269831694174e-06, Test loss 0.4421442780643702, Train accuracy 100.0, Test accuracy 94.53125, Cost 127.61313724517822 s\n",
      "Epoch 2070, Train loss 1.20668370327163e-06, Test loss 0.4421345226466656, Train accuracy 100.0, Test accuracy 94.53125, Cost 127.22904348373413 s\n",
      "Model saved in epoch 2070\n",
      "Epoch 2071, Train loss 1.1755822305365438e-06, Test loss 0.442166018858552, Train accuracy 100.0, Test accuracy 94.53125, Cost 126.60102796554565 s\n",
      "Epoch 2072, Train loss 9.691085296198556e-07, Test loss 0.44216799437999726, Train accuracy 100.0, Test accuracy 94.53125, Cost 127.01934456825256 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2073, Train loss 1.586394377467324e-06, Test loss 0.44219005554914476, Train accuracy 100.0, Test accuracy 94.521484375, Cost 127.13092732429504 s\n",
      "Epoch 2074, Train loss 1.2619997604939129e-06, Test loss 0.44216815419495104, Train accuracy 100.0, Test accuracy 94.53125, Cost 126.91448307037354 s\n",
      "Epoch 2075, Train loss 1.2511907821058321e-06, Test loss 0.4422134432941675, Train accuracy 100.0, Test accuracy 94.541015625, Cost 127.10126686096191 s\n",
      "Model saved in epoch 2075\n",
      "Epoch 2076, Train loss 1.4813247245345655e-06, Test loss 0.44217534437775613, Train accuracy 100.0, Test accuracy 94.541015625, Cost 127.29738688468933 s\n",
      "Epoch 2077, Train loss 1.138457041291735e-06, Test loss 0.4422131240367889, Train accuracy 100.0, Test accuracy 94.53125, Cost 126.75085234642029 s\n",
      "Epoch 2078, Train loss 1.1442674541319456e-06, Test loss 0.44221967160701753, Train accuracy 100.0, Test accuracy 94.53125, Cost 127.56202602386475 s\n",
      "Epoch 2079, Train loss 1.2392852739029574e-06, Test loss 0.4422542452812195, Train accuracy 100.0, Test accuracy 94.53125, Cost 127.38715076446533 s\n",
      "Epoch 2080, Train loss 1.1923753050321499e-06, Test loss 0.4423174452036619, Train accuracy 100.0, Test accuracy 94.541015625, Cost 127.0015881061554 s\n",
      "Model saved in epoch 2080\n",
      "Epoch 2081, Train loss 2.2270523122789953e-06, Test loss 0.4423660974949598, Train accuracy 100.0, Test accuracy 94.560546875, Cost 126.9264280796051 s\n",
      "Epoch 2082, Train loss 1.4895827764367284e-06, Test loss 0.4423420146107674, Train accuracy 100.0, Test accuracy 94.55078125, Cost 127.10302066802979 s\n",
      "Epoch 2083, Train loss 1.3766871668128923e-06, Test loss 0.4422354020178318, Train accuracy 100.0, Test accuracy 94.55078125, Cost 127.44426250457764 s\n",
      "Epoch 2084, Train loss 1.5929714397997235e-06, Test loss 0.44226070009171964, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.3212080001831 s\n",
      "Epoch 2085, Train loss 1.2278515019601149e-06, Test loss 0.4422613956034184, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.4290418624878 s\n",
      "Model saved in epoch 2085\n",
      "Epoch 2086, Train loss 3.955777022372679e-06, Test loss 0.44256474301218984, Train accuracy 100.0, Test accuracy 94.560546875, Cost 126.85114336013794 s\n",
      "Epoch 2087, Train loss 1.5411498063935088e-06, Test loss 0.44260752387344837, Train accuracy 100.0, Test accuracy 94.560546875, Cost 126.94482946395874 s\n",
      "Epoch 2088, Train loss 1.0145926143769338e-06, Test loss 0.44254415258765223, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.22935891151428 s\n",
      "Epoch 2089, Train loss 1.680044309521407e-06, Test loss 0.44284365996718406, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.37484097480774 s\n",
      "Epoch 2090, Train loss 1.1696014644954e-06, Test loss 0.44288725182414057, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.33197164535522 s\n",
      "Model saved in epoch 2090\n",
      "Epoch 2091, Train loss 1.2730483232841975e-06, Test loss 0.4429698631167412, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.15628385543823 s\n",
      "Epoch 2092, Train loss 1.158282988443306e-06, Test loss 0.4429781254380941, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.26309299468994 s\n",
      "Epoch 2093, Train loss 1.37418378260208e-06, Test loss 0.44303425811231134, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.44146728515625 s\n",
      "Epoch 2094, Train loss 1.7012753335985352e-06, Test loss 0.4429543107748032, Train accuracy 100.0, Test accuracy 94.55078125, Cost 127.92365527153015 s\n",
      "Epoch 2095, Train loss 1.2470658227430863e-06, Test loss 0.4429549310356379, Train accuracy 100.0, Test accuracy 94.55078125, Cost 127.68658304214478 s\n",
      "Model saved in epoch 2095\n",
      "Epoch 2096, Train loss 1.120067909614019e-06, Test loss 0.4428752686828375, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.45052361488342 s\n",
      "Epoch 2097, Train loss 1.2138839262110462e-06, Test loss 0.44293403476476667, Train accuracy 100.0, Test accuracy 94.55078125, Cost 126.90306162834167 s\n",
      "Epoch 2098, Train loss 1.5618580946840078e-06, Test loss 0.44301041811704633, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.89176154136658 s\n",
      "Epoch 2099, Train loss 2.503715309108502e-06, Test loss 0.4433225758373737, Train accuracy 100.0, Test accuracy 94.541015625, Cost 126.74542832374573 s\n",
      "Epoch 2100, Train loss 1.172341974844665e-06, Test loss 0.44334866516292093, Train accuracy 100.0, Test accuracy 94.53125, Cost 127.61114859580994 s\n",
      "Model saved in epoch 2100\n",
      "Epoch 2101, Train loss 1.793418718964763e-06, Test loss 0.4433358658105135, Train accuracy 100.0, Test accuracy 94.53125, Cost 127.8135974407196 s\n",
      "Epoch 2102, Train loss 2.0031240111798644e-06, Test loss 0.44325392730534074, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.63791465759277 s\n",
      "Epoch 2103, Train loss 1.099895693113653e-06, Test loss 0.44323692098259926, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.50678992271423 s\n",
      "Epoch 2104, Train loss 1.6028862101921726e-06, Test loss 0.4432044394314289, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.40484619140625 s\n",
      "Epoch 2105, Train loss 1.1887725694891474e-06, Test loss 0.44320326633751395, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.13778901100159 s\n",
      "Model saved in epoch 2105\n",
      "Epoch 2106, Train loss 1.2806975126386793e-06, Test loss 0.44324151761829855, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.34597897529602 s\n",
      "Epoch 2107, Train loss 1.4644920350791208e-06, Test loss 0.4432213794440031, Train accuracy 100.0, Test accuracy 94.55078125, Cost 127.15173602104187 s\n",
      "Epoch 2108, Train loss 1.2002209791062827e-06, Test loss 0.4431747429072857, Train accuracy 100.0, Test accuracy 94.55078125, Cost 126.82965397834778 s\n",
      "Epoch 2109, Train loss 1.1724595893704933e-06, Test loss 0.4431624788790941, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.12910771369934 s\n",
      "Epoch 2110, Train loss 1.098809825622491e-06, Test loss 0.4431354131549597, Train accuracy 100.0, Test accuracy 94.5703125, Cost 127.5515968799591 s\n",
      "Model saved in epoch 2110\n",
      "Epoch 2111, Train loss 1.2339258561498276e-06, Test loss 0.44315109103918077, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.21786642074585 s\n",
      "Epoch 2112, Train loss 1.5711272713755092e-06, Test loss 0.44313583001494405, Train accuracy 100.0, Test accuracy 94.560546875, Cost 126.85572719573975 s\n",
      "Epoch 2113, Train loss 2.2429562443822355e-06, Test loss 0.44337690211832526, Train accuracy 100.0, Test accuracy 94.5703125, Cost 127.45014715194702 s\n",
      "Epoch 2114, Train loss 1.5151330885431394e-06, Test loss 0.4434171989560127, Train accuracy 100.0, Test accuracy 94.5703125, Cost 127.74523758888245 s\n",
      "Epoch 2115, Train loss 1.0736893404293708e-06, Test loss 0.44344369769096376, Train accuracy 100.0, Test accuracy 94.5703125, Cost 127.0445556640625 s\n",
      "Model saved in epoch 2115\n",
      "Epoch 2116, Train loss 1.2714597927435292e-06, Test loss 0.4434158753603697, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.45417976379395 s\n",
      "Epoch 2117, Train loss 1.7286214099497345e-06, Test loss 0.4432955179363489, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.28125476837158 s\n",
      "Epoch 2118, Train loss 1.8994632997366609e-06, Test loss 0.4433705072849989, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.16168737411499 s\n",
      "Epoch 2119, Train loss 1.30020238031885e-06, Test loss 0.4433078620582819, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.5396900177002 s\n",
      "Epoch 2120, Train loss 1.7497811550124259e-06, Test loss 0.44337126091122625, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.28627634048462 s\n",
      "Model saved in epoch 2120\n",
      "Epoch 2121, Train loss 1.2204044436975428e-06, Test loss 0.4433439649641514, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.4505763053894 s\n",
      "Epoch 2122, Train loss 1.269834237334429e-06, Test loss 0.4434195563197136, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.2229266166687 s\n",
      "Epoch 2123, Train loss 9.525567152472402e-07, Test loss 0.443374590203166, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.24916982650757 s\n",
      "Epoch 2124, Train loss 1.391255286939227e-06, Test loss 0.4433632757514715, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.21834135055542 s\n",
      "Epoch 2125, Train loss 1.3321742568845793e-06, Test loss 0.44342471174895765, Train accuracy 100.0, Test accuracy 94.560546875, Cost 126.97324323654175 s\n",
      "Model saved in epoch 2125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2126, Train loss 1.2812084437275e-06, Test loss 0.44341500848531723, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.20696568489075 s\n",
      "Epoch 2127, Train loss 1.550563148571815e-06, Test loss 0.44340007193386555, Train accuracy 100.0, Test accuracy 94.580078125, Cost 126.71344137191772 s\n",
      "Epoch 2128, Train loss 2.586886456837073e-06, Test loss 0.4433384336531162, Train accuracy 100.0, Test accuracy 94.541015625, Cost 127.53823947906494 s\n",
      "Epoch 2129, Train loss 1.5265786981188967e-06, Test loss 0.4433367222547531, Train accuracy 100.0, Test accuracy 94.560546875, Cost 126.98248147964478 s\n",
      "Epoch 2130, Train loss 1.455077888165851e-06, Test loss 0.4433231681585312, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.49804258346558 s\n",
      "Model saved in epoch 2130\n",
      "Epoch 2131, Train loss 1.1277030095676531e-06, Test loss 0.4432827811688185, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.13890409469604 s\n",
      "Epoch 2132, Train loss 1.9404024583349604e-06, Test loss 0.4433399610221386, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.28623247146606 s\n",
      "Epoch 2133, Train loss 1.4136557763527168e-06, Test loss 0.44333922266960146, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.79917073249817 s\n",
      "Epoch 2134, Train loss 1.08145331443729e-06, Test loss 0.44338169172406194, Train accuracy 100.0, Test accuracy 94.5703125, Cost 127.36967444419861 s\n",
      "Epoch 2135, Train loss 1.7625330318503414e-06, Test loss 0.4432458646595478, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.23803114891052 s\n",
      "Model saved in epoch 2135\n",
      "Epoch 2136, Train loss 1.544140034005515e-06, Test loss 0.44331741109490397, Train accuracy 100.0, Test accuracy 94.58984375, Cost 126.9667911529541 s\n",
      "Epoch 2137, Train loss 1.7102502774886481e-06, Test loss 0.44334328509867194, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.40608954429626 s\n",
      "Epoch 2138, Train loss 1.2081736330566202e-06, Test loss 0.44334582686424256, Train accuracy 100.0, Test accuracy 94.58984375, Cost 126.42221689224243 s\n",
      "Epoch 2139, Train loss 9.68967559078493e-07, Test loss 0.4433471616357565, Train accuracy 100.0, Test accuracy 94.58984375, Cost 126.71515560150146 s\n",
      "Epoch 2140, Train loss 1.3116873332621732e-06, Test loss 0.4433911185711622, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.42258381843567 s\n",
      "Model saved in epoch 2140\n",
      "Epoch 2141, Train loss 1.3157256607531744e-06, Test loss 0.44337143301963805, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.59750080108643 s\n",
      "Epoch 2142, Train loss 1.1275612736657264e-06, Test loss 0.4433561637997627, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.45804357528687 s\n",
      "Epoch 2143, Train loss 1.0150230245470235e-06, Test loss 0.4433320123702288, Train accuracy 100.0, Test accuracy 94.580078125, Cost 126.64421725273132 s\n",
      "Epoch 2144, Train loss 1.0724407523121195e-06, Test loss 0.44334768280386927, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.41862750053406 s\n",
      "Epoch 2145, Train loss 1.0402176517116176e-06, Test loss 0.44328545071184633, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.17292904853821 s\n",
      "Model saved in epoch 2145\n",
      "Epoch 2146, Train loss 1.3287846035238613e-06, Test loss 0.44322430454194545, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.1902060508728 s\n",
      "Epoch 2147, Train loss 1.1065598205358844e-06, Test loss 0.4432797346264124, Train accuracy 100.0, Test accuracy 94.5703125, Cost 126.48506689071655 s\n",
      "Epoch 2148, Train loss 1.3276682305751288e-06, Test loss 0.44329559430480003, Train accuracy 100.0, Test accuracy 94.5703125, Cost 127.37476754188538 s\n",
      "Epoch 2149, Train loss 1.8686745681814665e-06, Test loss 0.44328153878450394, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.0453269481659 s\n",
      "Epoch 2150, Train loss 9.257001099538304e-07, Test loss 0.4433548580855131, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.21365547180176 s\n",
      "Model saved in epoch 2150\n",
      "Epoch 2151, Train loss 1.5142930825645322e-06, Test loss 0.44329271987080576, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.2815637588501 s\n",
      "Epoch 2152, Train loss 9.977987564717094e-07, Test loss 0.44326691441237925, Train accuracy 100.0, Test accuracy 94.580078125, Cost 126.95313715934753 s\n",
      "Epoch 2153, Train loss 1.1675095006642117e-06, Test loss 0.443310010433197, Train accuracy 100.0, Test accuracy 94.58984375, Cost 126.76570582389832 s\n",
      "Epoch 2154, Train loss 1.1665870395499144e-06, Test loss 0.44333361051976683, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.27794337272644 s\n",
      "Epoch 2155, Train loss 1.442908108659723e-06, Test loss 0.4433448325842619, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.29780507087708 s\n",
      "Model saved in epoch 2155\n",
      "Epoch 2156, Train loss 1.0887968967962796e-06, Test loss 0.443390054628253, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.18196749687195 s\n",
      "Epoch 2157, Train loss 1.09280349156199e-06, Test loss 0.4433870356529951, Train accuracy 100.0, Test accuracy 94.58984375, Cost 126.78360366821289 s\n",
      "Epoch 2158, Train loss 9.022413231223869e-07, Test loss 0.4434175129979849, Train accuracy 100.0, Test accuracy 94.58984375, Cost 126.713698387146 s\n",
      "Epoch 2159, Train loss 9.550947558820416e-07, Test loss 0.44342340268194674, Train accuracy 100.0, Test accuracy 94.58984375, Cost 126.4800751209259 s\n",
      "Epoch 2160, Train loss 8.814776962954138e-07, Test loss 0.4434190567582846, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.78389048576355 s\n",
      "Model saved in epoch 2160\n",
      "Epoch 2161, Train loss 1.2117830894004091e-06, Test loss 0.4434302531182766, Train accuracy 100.0, Test accuracy 94.58984375, Cost 126.68489956855774 s\n",
      "Epoch 2162, Train loss 7.91646182933678e-07, Test loss 0.4434626005589962, Train accuracy 100.0, Test accuracy 94.58984375, Cost 128.04150223731995 s\n",
      "Epoch 2163, Train loss 9.23798554917949e-07, Test loss 0.44345986619591715, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.41789937019348 s\n",
      "Epoch 2164, Train loss 1.5169826262996482e-06, Test loss 0.4436085164546967, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.27696633338928 s\n",
      "Epoch 2165, Train loss 8.547521532107089e-07, Test loss 0.4436322212219238, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.13805842399597 s\n",
      "Model saved in epoch 2165\n",
      "Epoch 2166, Train loss 1.2669035353358862e-06, Test loss 0.44363243617117404, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.5909218788147 s\n",
      "Epoch 2167, Train loss 9.74592380195308e-07, Test loss 0.44363462924957275, Train accuracy 100.0, Test accuracy 94.58984375, Cost 127.70495676994324 s\n",
      "Epoch 2168, Train loss 1.4204147548225992e-06, Test loss 0.44374374113976955, Train accuracy 100.0, Test accuracy 94.599609375, Cost 127.15936183929443 s\n",
      "Epoch 2169, Train loss 2.655567515455198e-06, Test loss 0.4439313817769289, Train accuracy 100.0, Test accuracy 94.5703125, Cost 127.92749571800232 s\n",
      "Epoch 2170, Train loss 1.829030050517307e-06, Test loss 0.44393943846225736, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.34044170379639 s\n",
      "Model saved in epoch 2170\n",
      "Epoch 2171, Train loss 1.111869460510743e-06, Test loss 0.44389875084161756, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.13290429115295 s\n",
      "Epoch 2172, Train loss 9.358548790157435e-07, Test loss 0.44389610961079595, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.36964797973633 s\n",
      "Epoch 2173, Train loss 1.3214041252046618e-06, Test loss 0.44391091652214526, Train accuracy 100.0, Test accuracy 94.560546875, Cost 126.72199010848999 s\n",
      "Epoch 2174, Train loss 1.3555878579742534e-06, Test loss 0.44391390308737755, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.80826091766357 s\n",
      "Epoch 2175, Train loss 1.5025153535955634e-06, Test loss 0.4439269553869963, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.88250041007996 s\n",
      "Model saved in epoch 2175\n",
      "Epoch 2176, Train loss 1.0513739729979063e-06, Test loss 0.4439909033477306, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.15364718437195 s\n",
      "Epoch 2177, Train loss 1.0809765507928535e-06, Test loss 0.4438984408974648, Train accuracy 100.0, Test accuracy 94.5703125, Cost 126.78289246559143 s\n",
      "Epoch 2178, Train loss 8.085496921201545e-07, Test loss 0.4438698381185532, Train accuracy 100.0, Test accuracy 94.5703125, Cost 127.8021240234375 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2179, Train loss 1.4018379341286269e-06, Test loss 0.4440273966640234, Train accuracy 100.0, Test accuracy 94.5703125, Cost 127.287766456604 s\n",
      "Epoch 2180, Train loss 1.5466177622231244e-06, Test loss 0.44420640394091604, Train accuracy 100.0, Test accuracy 94.5703125, Cost 127.37788081169128 s\n",
      "Model saved in epoch 2180\n",
      "Epoch 2181, Train loss 1.0993357596535107e-06, Test loss 0.44419694393873216, Train accuracy 100.0, Test accuracy 94.560546875, Cost 127.29351997375488 s\n",
      "Epoch 2182, Train loss 1.266793141995159e-06, Test loss 0.44427356123924255, Train accuracy 100.0, Test accuracy 94.5703125, Cost 127.49633264541626 s\n",
      "Epoch 2183, Train loss 1.086663701377786e-06, Test loss 0.4442587301135063, Train accuracy 100.0, Test accuracy 94.5703125, Cost 127.42353415489197 s\n",
      "Epoch 2184, Train loss 1.1038601745914497e-06, Test loss 0.44423845782876015, Train accuracy 100.0, Test accuracy 94.5703125, Cost 127.72505831718445 s\n",
      "Epoch 2185, Train loss 1.3914074342407871e-06, Test loss 0.44415077976882456, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.60322952270508 s\n",
      "Model saved in epoch 2185\n",
      "Epoch 2186, Train loss 2.603885851463727e-06, Test loss 0.44442671313881876, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.15420341491699 s\n",
      "Epoch 2187, Train loss 9.127495185065218e-07, Test loss 0.44434825219213964, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.47569942474365 s\n",
      "Epoch 2188, Train loss 1.0631792534970932e-06, Test loss 0.44436645582318307, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.42514085769653 s\n",
      "Epoch 2189, Train loss 1.3182453825404032e-06, Test loss 0.4444068893790245, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.44735598564148 s\n",
      "Epoch 2190, Train loss 1.2321211348431898e-06, Test loss 0.4443967256695032, Train accuracy 100.0, Test accuracy 94.580078125, Cost 126.9395318031311 s\n",
      "Model saved in epoch 2190\n",
      "Epoch 2191, Train loss 1.1953206860959122e-06, Test loss 0.4443712465465069, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.33918142318726 s\n",
      "Epoch 2192, Train loss 9.770204159088377e-07, Test loss 0.4444745734333992, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.12836527824402 s\n",
      "Epoch 2193, Train loss 1.421031624489058e-06, Test loss 0.4445158377289772, Train accuracy 100.0, Test accuracy 94.5703125, Cost 126.78618669509888 s\n",
      "Epoch 2194, Train loss 8.579506654231663e-07, Test loss 0.4445031818002462, Train accuracy 100.0, Test accuracy 94.5703125, Cost 127.79214477539062 s\n",
      "Epoch 2195, Train loss 1.096347298641976e-06, Test loss 0.4444754678755999, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.23834753036499 s\n",
      "Model saved in epoch 2195\n",
      "Epoch 2196, Train loss 1.0252813484742995e-06, Test loss 0.4445075705647469, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.71073293685913 s\n",
      "Epoch 2197, Train loss 1.1380152879186244e-06, Test loss 0.44450764209032056, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.36444115638733 s\n",
      "Epoch 2198, Train loss 1.2974869599586281e-06, Test loss 0.44455160908401015, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.11224722862244 s\n",
      "Epoch 2199, Train loss 1.4851687810543746e-06, Test loss 0.444620456174016, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.36775088310242 s\n",
      "Epoch 2200, Train loss 1.052953028994976e-06, Test loss 0.44466093070805074, Train accuracy 100.0, Test accuracy 94.580078125, Cost 126.91497993469238 s\n",
      "Model saved in epoch 2200\n",
      "Epoch 2201, Train loss 9.888566846086922e-07, Test loss 0.44464885592460635, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.2733952999115 s\n",
      "Epoch 2202, Train loss 8.631354452757835e-07, Test loss 0.4446470409631729, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.40343260765076 s\n",
      "Epoch 2203, Train loss 1.697504118294778e-06, Test loss 0.444668086245656, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.28220200538635 s\n",
      "Epoch 2204, Train loss 1.1687621520164721e-06, Test loss 0.44461703263223173, Train accuracy 100.0, Test accuracy 94.580078125, Cost 126.92126822471619 s\n",
      "Epoch 2205, Train loss 1.4293345874320938e-06, Test loss 0.4446835421025753, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.29548907279968 s\n",
      "Model saved in epoch 2205\n",
      "Epoch 2206, Train loss 1.0694878631363111e-06, Test loss 0.4446095686405897, Train accuracy 100.0, Test accuracy 94.580078125, Cost 127.47525215148926 s\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5000 # param\n",
    "epoch_start = 0\n",
    "# path = 'adam_rotate_center_crop1.pt'\n",
    "# path = 'block_3.pt'\n",
    "path = 'lr_0.1-0.0003_no_crop_decay_channel_80.pt'\n",
    "\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "test_accuracy_history = []\n",
    "train_accuracy_history = []\n",
    "\n",
    "Loss = torch.nn.CrossEntropyLoss()\n",
    "lr = 0.1 # param\n",
    "lr_min=0.0003\n",
    "# optimizer = torch.optim.SGD(model1.parameters(),lr=lr,momentum=0.9,weight_decay=5e-4) # changable optimizer\n",
    "# optimizer = torch.optim.SGD(model1.parameters(),lr=lr,momentum=0.9) # changable optimizer\n",
    "# optimizer = torch.optim.Adam(model1.parameters(),lr=lr, betas=(0.9,0.999), eps=1e-08, amsgrad=False) # changable optimizer\n",
    "momentum = 0.9\n",
    "nesterov = True\n",
    "optimizer = torch.optim.SGD(model1.parameters(),lr=lr,momentum=momentum,nesterov=nesterov)\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "if os.path.exists(path):\n",
    "  checkpoint = torch.load(path)\n",
    "  print('Read model from checkpoint')\n",
    "  model1.cuda().load_state_dict(checkpoint['model_state_dict'])\n",
    "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "  epoch_start = checkpoint['epoch']\n",
    "  Loss = checkpoint['Loss']\n",
    "  train_loss_history = checkpoint['train_loss_history']\n",
    "  test_loss_history = checkpoint['test_loss_history']\n",
    "  test_accuracy_history = checkpoint['test_accuracy_history']\n",
    "  train_accuracy_history = checkpoint['train_accuracy_history']\n",
    "  print('Restart from epoch',epoch_start)\n",
    "    \n",
    "\n",
    "for epoch in range(epoch_start+1, num_epochs + 1):\n",
    "  timestart = time.time()\n",
    "\n",
    "  train_loss = 0.0\n",
    "  test_loss = 0.0\n",
    "  test_accuracy = 0.0\n",
    "  train_accuracy = 0.0\n",
    "\n",
    "  for i, data in enumerate(trainDataLoader):\n",
    "    images, labels = data\n",
    "    images = images.cuda()\n",
    "    labels = labels.cuda()\n",
    "    optimizer.zero_grad()\n",
    "    predicted_output = model1.cuda()(images)\n",
    "    fit = Loss(predicted_output,labels)\n",
    "    fit.backward()\n",
    "    adjust_learning_rate(optimizer=optimizer,current_epoch=epoch,max_epoch=num_epochs,lr_min=lr_min,lr_max=lr,warmup=True)\n",
    "    optimizer.step()\n",
    "    train_loss += fit.item()\n",
    "    train_accuracy += (torch.eq(torch.max(predicted_output,1)[1],labels).sum()/len(labels)*100).data.cpu().numpy()\n",
    "\n",
    "  for i, data in enumerate(testDataLoader):\n",
    "    with torch.no_grad():\n",
    "      images, labels = data\n",
    "      images = images.cuda()\n",
    "      labels = labels.cuda()\n",
    "      predicted_output = model1.cuda()(images)\n",
    "      fit = Loss(predicted_output,labels)\n",
    "      test_loss += fit.item()\n",
    "      test_accuracy += (torch.eq(torch.max(predicted_output,1)[1],labels).sum()/len(labels)*100).data.cpu().numpy()\n",
    "\n",
    "\n",
    "  train_loss = train_loss/len(trainDataLoader)\n",
    "  test_loss = test_loss/len(testDataLoader)\n",
    "  test_accu = test_accuracy/len(testDataLoader)\n",
    "  train_accu = train_accuracy/len(trainDataLoader)\n",
    "  train_loss_history.append(train_loss)\n",
    "  test_loss_history.append(test_loss)\n",
    "  test_accuracy_history.append(test_accu)\n",
    "  train_accuracy_history.append(train_accu)\n",
    "  print('Epoch %s, Train loss %s, Test loss %s, Train accuracy %s, Test accuracy %s, Cost %s s'%(epoch,\n",
    "                                                                                                   train_loss,test_loss,\n",
    "                                                                                                   train_accu,test_accu,\n",
    "                                                                                                   time.time()-timestart))\n",
    "  \n",
    "  if epoch % 5 == 0 and epoch != 0:\n",
    "    torch.save({'epoch':epoch,\n",
    "          'model_state_dict':model1.cuda().state_dict(),\n",
    "          'optimizer_state_dict':optimizer.state_dict(),\n",
    "          'Loss':Loss,\n",
    "          'train_loss_history':train_loss_history,\n",
    "          'test_loss_history':test_loss_history,\n",
    "          'test_accuracy_history':test_accuracy_history,\n",
    "          'train_accuracy_history':train_accuracy_history},path)\n",
    "    print('Model saved in epoch %s'%(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUy08Iyn7tUl",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 1341 \n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(num_epochs),train_loss_history,'-',linewidth=3,label='Train error')\n",
    "plt.plot(range(num_epochs),test_loss_history,'-',linewidth=3,label='Test error')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(num_epochs),train_accuracy_history,'-',linewidth=3,label='Train accuracy')\n",
    "plt.plot(range(num_epochs),test_accuracy_history,'-',linewidth=3,label='Test accuracy')\n",
    "# plt.plot(range(num_epochs),test_accuracy_history,'-',linewidth=3,label='Test accuracy')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVQJgMts7vcg"
   },
   "outputs": [],
   "source": [
    "print('Accuracy:',sum(test_accuracy_history[-5:])/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LaMUB4p_Ucip"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "“ResNet.ipynb”的副本",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0130588af6254c76a2c8c382288cfcea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4aa5668c8eaa49f88148d499240b6ea5",
      "placeholder": "​",
      "style": "IPY_MODEL_a66b114e4595419f84ac44a676a1ca61",
      "value": " 170499072/? [00:03&lt;00:00, 56047078.45it/s]"
     }
    },
    "1121692d91114d58b3db79e41b0a24c9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d40f4bf0c0e48709cdec5f80069ed2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "321ea7fa0b4d4c9dab9ed5231954e54c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3983cfe844f847899487b2745a203a9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8148f05b38d94ec19351ac920b70acba",
       "IPY_MODEL_af7b660f147d4d1d8796d3419673c9b7",
       "IPY_MODEL_0130588af6254c76a2c8c382288cfcea"
      ],
      "layout": "IPY_MODEL_1121692d91114d58b3db79e41b0a24c9"
     }
    },
    "4aa5668c8eaa49f88148d499240b6ea5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8148f05b38d94ec19351ac920b70acba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_accf52fe298b4716af9d9c351e7326dc",
      "placeholder": "​",
      "style": "IPY_MODEL_fb3e769eb8324af3b77041879c9b54cf",
      "value": ""
     }
    },
    "a66b114e4595419f84ac44a676a1ca61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "accf52fe298b4716af9d9c351e7326dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af7b660f147d4d1d8796d3419673c9b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_321ea7fa0b4d4c9dab9ed5231954e54c",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1d40f4bf0c0e48709cdec5f80069ed2f",
      "value": 170498071
     }
    },
    "fb3e769eb8324af3b77041879c9b54cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
