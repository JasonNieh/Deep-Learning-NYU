{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "azOQDGfMLFN8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import math\n",
    "import torchvision\n",
    "from torchvision import transforms as transforms\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from math import cos,pi\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nnaef49GOhPH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Vb_Gr3w9vzx8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x14dd1403f970>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(17)\n",
    "\n",
    "class HaS(object): \n",
    "#     def __init__(self):\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        # get width and height of the image\n",
    "        img_= np.array(img).copy()\n",
    "        s = img_.shape\n",
    "        wd = s[0]\n",
    "        ht = s[1]\n",
    "\n",
    "        # possible grid size, 0 means no hiding\n",
    "        grid_size=3\n",
    "\n",
    "        # hiding probability\n",
    "        hide_prob = 0.1\n",
    " \n",
    "        # randomly choose one grid size\n",
    "#         grid_size= grid_sizes[random.randint(0,len(grid_sizes)-1)]\n",
    "\n",
    "        # hide the patches\n",
    "        if(grid_size>0):\n",
    "             for x in range(0,wd,grid_size):\n",
    "                 for y in range(0,ht,grid_size):\n",
    "                     x_end = min(wd, x+grid_size)  \n",
    "                     y_end = min(ht, y+grid_size)\n",
    "                     if(random.random() <=  hide_prob):\n",
    "                           img_[x:x_end,y:y_end,:]=0\n",
    "\n",
    "        return img_\n",
    "    \n",
    "torch.manual_seed(17)\n",
    "\n",
    "        \n",
    "# class HideEdge(object): \n",
    "#     def __init__(self,hide_size):\n",
    "#         self.hide_size=hide_size\n",
    "        \n",
    "#     def __call__(self, img):\n",
    "#         # get width and height of the image\n",
    "#         img_= np.array(img).copy()\n",
    "#         s = img_.shape\n",
    "#         wd = s[0]\n",
    "#         ht = s[1]\n",
    "\n",
    "#         hide_size=self.hide_size\n",
    "        \n",
    "# #         img_[:,:,:] = img()\n",
    "   \n",
    "#         x_end = wd - hide_size \n",
    "#         y_end = ht - hide_size\n",
    "\n",
    "#         img_[x_end:,y_end:,:]=0\n",
    "# #         img_[x_end:,:hide_size,:]=0\n",
    "# #         img_[:hide_size,y_end:,:]=0\n",
    "#         img_[:hide_size,:hide_size,:]=0\n",
    "# #         img_[x_end:,:,:]=0\n",
    "# #         img_[:,y_end:,:]=0\n",
    "# #         img_[:hide_size,:,:]=0\n",
    "# #         img_[:,:hide_size,:]=0\n",
    "# #         print(img_[x_end,y_end,:])\n",
    "# #         print(img_[hide_size,hide_size,:])\n",
    "# #         print(x_end,y_end,hide_size)\n",
    "        \n",
    "# #         mean = img_[hide_size:x_end-1,hide_size:y_end,:].mean()\n",
    "# #         std = img_[hide_size:x_end-1,hide_size:y_end,:].std()\n",
    "# #         print(mean, std)\n",
    "# #         img_[hide_size:x_end-1,hide_size:y_end,:] = (img_[hide_size:x_end-1,hide_size:y_end,:] - mean) / std\n",
    "# #         print(img_[hide_size:x_end-1,hide_size:y_end,:])\n",
    "        \n",
    "#         return img_\n",
    "\n",
    "   \n",
    "# class Hide_after_Norm(object): \n",
    "#     def __init__(self,hide_size):\n",
    "#         self.hide_size=hide_size\n",
    "        \n",
    "#     def __call__(self, img_):\n",
    "#         # get width and height of the image\n",
    "# #         img_= np.array(img).copy()\n",
    "#         s = img_.shape\n",
    "#         wd = s[1]\n",
    "#         ht = s[2]\n",
    "\n",
    "#         hide_size=self.hide_size\n",
    "        \n",
    "# #         img_[:,:,:] = img()\n",
    "   \n",
    "#         x_end = wd - hide_size \n",
    "#         y_end = ht - hide_size\n",
    "        \n",
    "#         x_end = wd - hide_size \n",
    "#         y_end = ht - hide_size\n",
    "\n",
    "#         img_[:,x_end:,y_end:]=0\n",
    "# #         img_[x_end:,:hide_size,:]=0\n",
    "# #         img_[:hide_size,y_end:,:]=0\n",
    "#         img_[:,:hide_size,:hide_size]=0\n",
    "# #         print(img_[x_end,y_end,:])\n",
    "# #         print(img_[hide_size,hide_size,:])\n",
    "# #         print(x_end,y_end,hide_size)\n",
    "        \n",
    "# #         mean = img_[hide_size:x_end-1,hide_size:y_end,:].mean()\n",
    "# #         std = img_[hide_size:x_end-1,hide_size:y_end,:].std()\n",
    "# #         print(mean, std)\n",
    "# #         img_[hide_size:x_end-1,hide_size:y_end,:] = (img_[hide_size:x_end-1,hide_size:y_end,:] - mean) / std\n",
    "# #         print(img_[hide_size:x_end-1,hide_size:y_end,:])\n",
    "        \n",
    "#         return img_\n",
    "    \n",
    "    \n",
    "\n",
    "# # torch.cuda.manual_seed(17) # for GPU\n",
    "# aug_train = transforms.Compose([\n",
    "#     transforms.RandomHorizontalFlip(), # 水平翻转\n",
    "# #     torchvision.transforms.CenterCrop(26),\n",
    "# #     HideEdge(),\n",
    "#     torchvision.transforms.RandomRotation(15),\n",
    "# #     torchvision.transforms.CenterCrop(28),\n",
    "#     # transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5), # color aug\n",
    "# #     transforms.RandomCrop(32, padding=4), # 裁剪\n",
    "#     # transforms.RandomResizedCrop((32,32),scale=(0.1,1),ratio=(0.5,2))\n",
    "# #     hide_patch(),\n",
    "# #     HaS(),\n",
    "# #     HideEdge(2),\n",
    "#     transforms.ToTensor(),\n",
    "# #     Norm(2),\n",
    "#     transforms.Normalize((0.4649, 0.4553, 0.4214), (0.2271, 0.2234, 0.2208)),# normalization\n",
    "#     Hide_after_Norm(2)\n",
    "#     ])\n",
    "\n",
    "# aug_test = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.4649, 0.4553, 0.4214), (0.2271, 0.2234, 0.2208)), # normalization\n",
    "#     Hide_after_Norm(2)\n",
    "#     ])\n",
    "\n",
    "# trainingdata = torchvision.datasets.CIFAR10('./CIFAR10',train=True,download=True,transform=aug_train)\n",
    "# # testdata = torchvision.datasets.CIFAR10('./CIFAR10',train=False,download=True,transform=transforms.ToTensor())\n",
    "# # print(len(trainingdata),len(testdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(17)\n",
    "torch.cuda.manual_seed_all(17)\n",
    "\n",
    "aug_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32,padding=4,padding_mode='reflect'),\n",
    "    transforms.RandomHorizontalFlip(), # 水平翻转\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4244, 0.4146, 0.3836), (0.2539, 0.2491, 0.2420)) # normalization\n",
    "    ])\n",
    "\n",
    "aug_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4244, 0.4146, 0.3836), (0.2539, 0.2491, 0.2420)) # normalization\n",
    "    ])\n",
    "\n",
    "trainingdata = torchvision.datasets.CIFAR10('./CIFAR10',train=True,download=True,transform=aug_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103,
     "referenced_widgets": [
      "3983cfe844f847899487b2745a203a9b",
      "1121692d91114d58b3db79e41b0a24c9",
      "8148f05b38d94ec19351ac920b70acba",
      "af7b660f147d4d1d8796d3419673c9b7",
      "0130588af6254c76a2c8c382288cfcea",
      "fb3e769eb8324af3b77041879c9b54cf",
      "accf52fe298b4716af9d9c351e7326dc",
      "1d40f4bf0c0e48709cdec5f80069ed2f",
      "321ea7fa0b4d4c9dab9ed5231954e54c",
      "a66b114e4595419f84ac44a676a1ca61",
      "4aa5668c8eaa49f88148d499240b6ea5"
     ]
    },
    "id": "1lqsbqYCMja7",
    "outputId": "3b4aa629-06a0-480a-afec-dcdb971e4bf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def get_mean_and_std(dataset):\n",
    "  '''Compute the mean and std value of dataset.'''\n",
    "  dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "  mean = torch.zeros(3)\n",
    "  std = torch.zeros(3)\n",
    "  print('==> Computing mean and std..')\n",
    "  for inputs, targets in dataloader:\n",
    "      for i in range(3):\n",
    "          mean[i] += inputs[:,i,:,:].mean()\n",
    "          std[i] += inputs[:,i,:,:].std()\n",
    "  mean.div_(len(dataset))\n",
    "  std.div_(len(dataset))\n",
    "  return mean, std\n",
    "\n",
    "def load_data(is_train,aug,batch_size):\n",
    "  dataset = torchvision.datasets.CIFAR10('./CIFAR10',train=is_train,download=True,transform=aug)\n",
    "#   mean, std = get_mean_and_std(dataset)\n",
    "#   print(mean, std)\n",
    "  dataloader = torch.utils.data.DataLoader(dataset,batch_size=batch_size,shuffle=is_train)\n",
    "  return dataloader\n",
    "\n",
    "batch_size = 128 # param\n",
    "trainDataLoader = load_data(is_train=True,aug=aug_train,batch_size=batch_size)\n",
    "testDataLoader = load_data(is_train=False,aug=aug_test,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-TD4UKtgzXbh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32]) 6\n",
      "[[[-0.9147032  -0.38956204  0.0274618  ...  0.01201648  0.18191507\n",
      "    0.15102442]\n",
      "  [-0.32778072 -0.03431951  0.10468844 ... -0.21966343  0.1973604\n",
      "    0.2591417 ]\n",
      "  [-0.09610081  0.29003236  0.24369638 ... -0.6366873  -0.51312464\n",
      "   -0.32778072]\n",
      "  ...\n",
      "  [ 1.0931895   1.2321974   1.448432   ...  0.22825105  0.66072035\n",
      "    1.6646665 ]\n",
      "  [ 1.2013068   1.3248694   1.3866507  ... -0.38956204 -0.17332745\n",
      "    1.1704161 ]\n",
      "  [ 1.3866507   1.2785335   1.1549708  ... -0.8529219  -0.8065859\n",
      "    0.7997283 ]]\n",
      "\n",
      " [[-1.2393323  -0.8772444  -0.53089947 ... -0.5151565  -0.3419841\n",
      "   -0.3419841 ]\n",
      "  [-0.8142726  -0.6725861  -0.5623854  ... -0.68832904 -0.32624117\n",
      "   -0.29475525]\n",
      "  [-0.6411001  -0.42069885 -0.45218474 ... -1.003188   -0.9244732\n",
      "   -0.76704377]\n",
      "  ...\n",
      "  [ 0.57110703  0.6813077   0.9804237  ... -0.21604052  0.19327614\n",
      "    1.2323109 ]\n",
      "  [ 0.6025929   0.74427944  0.8229942  ... -0.83001554 -0.68832904\n",
      "    0.6655647 ]\n",
      "  [ 0.87022305  0.8072512   0.6340788  ... -1.1291317  -1.1763605\n",
      "    0.42942047]]\n",
      "\n",
      " [[-1.4554853  -1.212413   -0.92072594 ... -0.9045211  -0.7748825\n",
      "   -0.7748825 ]\n",
      "  [-1.1800033  -1.1313888  -1.0503646  ... -1.017955   -0.80729216\n",
      "   -0.7748825 ]\n",
      "  [-1.0341598  -0.9531356  -1.0017501  ... -1.1800033  -1.212413\n",
      "   -1.1475936 ]\n",
      "  ...\n",
      "  [-0.17530379 -0.49940035 -0.4831955  ... -0.4183762  -0.22391827\n",
      "    0.6835522 ]\n",
      "  [-1.0989791  -1.1800033  -1.0341598  ... -1.0341598  -1.0341598\n",
      "   -0.06186999]\n",
      "  [-1.1637985  -1.1475936  -1.0341598  ... -1.2610275  -1.4716902\n",
      "   -0.45078588]]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "image,label = trainingdata[0]\n",
    "image_= np.array(image).copy()\n",
    "print(image.shape, label)\n",
    "print(image_)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.imshow(image.numpy().transpose(1,2,0))\n",
    "plt.title(str(classes[label]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jo4vcA1BwajW"
   },
   "outputs": [],
   "source": [
    "# trainDataLoader = torch.utils.data.DataLoader(trainingdata,batch_size=batch_size,shuffle=True)\n",
    "# testDataLoader = torch.utils.data.DataLoader(testdata,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "# images, labels = iter(trainDataLoader).next()\n",
    "# plt.figure(figsize=(17,8))\n",
    "# for index in np.arange(0,5):\n",
    "#   plt.subplot(1,5,index+1)\n",
    "#   plt.imshow(images[index].numpy().transpose(1,2,0))\n",
    "#   plt.title(str(classes[labels[index]]))\n",
    "\n",
    "def get_mean_and_std(dataset):\n",
    "  '''Compute the mean and std value of dataset.'''\n",
    "  dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "  mean = torch.zeros(3)\n",
    "  std = torch.zeros(3)\n",
    "  print('==> Computing mean and std..')\n",
    "  for inputs, targets in dataloader:\n",
    "      for i in range(3):\n",
    "          mean[i] += inputs[:,i,:,:].mean()\n",
    "          std[i] += inputs[:,i,:,:].std()\n",
    "  mean.div_(len(dataset))\n",
    "  std.div_(len(dataset))\n",
    "  return mean, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YDBTjSf2jDNm"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, in_planes, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = in_planes\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, self.in_planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_planes)\n",
    "        self.layer1 = self._make_layer(block, in_planes, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, in_planes*2, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, in_planes*4, num_blocks[2], stride=2)\n",
    "#         self.layer4 = self._make_layer(block, in_planes*8, num_blocks[3], stride=2)\n",
    "#         self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "#         self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "#         self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "#         self.layer4 = self._make_layer(block, self.in_planes*8, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "#         print(out.shape)\n",
    "        out = self.layer1(out)\n",
    "#         print(out.shape)\n",
    "        out = self.layer2(out)\n",
    "#         print(out.shape)\n",
    "        out = self.layer3(out)\n",
    "#         print(out.shape)\n",
    "#         out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "#         print(out.shape)\n",
    "        out = out.view(out.size(0), -1)\n",
    "#         print(out.shape)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight,mode='fan_out',nonlinearity='relu') # weight initialization\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias,0)\n",
    "            elif isinstance(m,nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight,1)\n",
    "                nn.init.constant_(m.bias,0)\n",
    "            elif isinstance(m,nn.Linear):\n",
    "                nn.init.normal_(m.weight,std=1e-3)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias,0)\n",
    "\n",
    "def project1_model():\n",
    "#     return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "#     return ResNet(BasicBlock, [2, 2, 2])\n",
    "    return ResNet(64, BasicBlock, [3, 3, 3])\n",
    "\n",
    "# model1 = nn.Sequential(project1_model(), nn.AdaptiveAvgPool2d((1,1)), nn.Flatten(), nn.Linear(512, 10)).cuda()\n",
    "model1 = project1_model().cuda()\n",
    "model1.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FDnI9zbyLK6B",
    "outputId": "4a1b7b4e-5d52-42d6-8563-500023c3acae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335434\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    # torch.numel() returns number of elements in a tensor\n",
    "\n",
    "print(count_parameters(model1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, current_epoch,max_epoch,lr_min=0,lr_max=0.1,warmup=True):\n",
    "    warmup_epoch = 10 if warmup else 0\n",
    "    if current_epoch < warmup_epoch:\n",
    "        lr = lr_max * current_epoch / warmup_epoch\n",
    "    else:\n",
    "        lr = lr_min + (lr_max-lr_min)*(1 + cos(pi * (current_epoch - warmup_epoch) / (max_epoch - warmup_epoch))) / 2\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HbLtFYydjoIx",
    "outputId": "0321ff7b-c60c-4099-f5fb-4a9536f10244"
   },
   "outputs": [],
   "source": [
    "# X = torch.rand(size=(1, 3, 32, 32)).cuda()\n",
    "# for layer in model1:\n",
    "#   X = layer(X)\n",
    "#   print(layer.__class__.__name__,'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "id": "j5xklXYe6gRe",
    "outputId": "b846ff14-cc67-4bdc-d6bb-f727fa580bcc",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read model from checkpoint\n",
      "Restart from epoch 1700\n",
      "Epoch 1701, Train loss 7.739918532986174e-06, Test loss 0.5536197941703133, Train accuracy 100.0, Test accuracy 93.92800632911393, Cost 78.0152907371521 s\n",
      "Epoch 1702, Train loss 1.3336263703112397e-05, Test loss 0.5591882418228101, Train accuracy 100.0, Test accuracy 93.9181170886076, Cost 78.06867241859436 s\n",
      "Epoch 1703, Train loss 6.724497964798562e-06, Test loss 0.5602872442406944, Train accuracy 100.0, Test accuracy 93.92800632911393, Cost 77.89485025405884 s\n",
      "Epoch 1704, Train loss 6.018022605337427e-05, Test loss 0.5525626578851591, Train accuracy 99.99800191815856, Test accuracy 94.07634493670886, Cost 78.1093270778656 s\n",
      "Epoch 1705, Train loss 1.1203940107878142e-05, Test loss 0.5506820327873472, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 77.92202949523926 s\n",
      "Model saved in epoch 1705\n",
      "Epoch 1706, Train loss 9.563755504678156e-06, Test loss 0.5496814387697208, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.19778752326965 s\n",
      "Epoch 1707, Train loss 7.215135589475312e-06, Test loss 0.5518951973394502, Train accuracy 100.0, Test accuracy 94.03678797468355, Cost 77.87696409225464 s\n",
      "Epoch 1708, Train loss 8.165330544366264e-06, Test loss 0.548528999278817, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 77.97839069366455 s\n",
      "Epoch 1709, Train loss 3.5757745327025872e-06, Test loss 0.5487539915344383, Train accuracy 100.0, Test accuracy 94.04667721518987, Cost 77.76457214355469 s\n",
      "Epoch 1710, Train loss 4.523821533662593e-06, Test loss 0.5477922243785255, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 77.9767553806305 s\n",
      "Model saved in epoch 1710\n",
      "Epoch 1711, Train loss 7.793617327463131e-06, Test loss 0.5478107326581508, Train accuracy 100.0, Test accuracy 94.1257911392405, Cost 77.93933653831482 s\n",
      "Epoch 1712, Train loss 5.808254333591001e-06, Test loss 0.5479817435711245, Train accuracy 100.0, Test accuracy 94.11590189873418, Cost 78.10581803321838 s\n",
      "Epoch 1713, Train loss 8.98461516342847e-06, Test loss 0.5490506853483901, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 77.94576120376587 s\n",
      "Epoch 1714, Train loss 5.036044768225253e-06, Test loss 0.548127219080925, Train accuracy 100.0, Test accuracy 94.11590189873418, Cost 78.06962275505066 s\n",
      "Epoch 1715, Train loss 4.154360441980501e-06, Test loss 0.5478947498375857, Train accuracy 100.0, Test accuracy 94.11590189873418, Cost 77.94269847869873 s\n",
      "Model saved in epoch 1715\n",
      "Epoch 1716, Train loss 4.060703321509405e-06, Test loss 0.5475872027911718, Train accuracy 100.0, Test accuracy 94.11590189873418, Cost 78.07617425918579 s\n",
      "Epoch 1717, Train loss 4.123950841283112e-05, Test loss 0.5480900515295282, Train accuracy 99.99800191815856, Test accuracy 94.17523734177215, Cost 77.88935017585754 s\n",
      "Epoch 1718, Train loss 5.5431601591422245e-05, Test loss 0.5507301994705502, Train accuracy 99.99800191815856, Test accuracy 94.1554588607595, Cost 78.14854598045349 s\n",
      "Epoch 1719, Train loss 1.5814202352113796e-05, Test loss 0.5486349471007721, Train accuracy 100.0, Test accuracy 94.1257911392405, Cost 77.75129175186157 s\n",
      "Epoch 1720, Train loss 1.1077537138376972e-05, Test loss 0.5481334396555454, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 78.0135452747345 s\n",
      "Model saved in epoch 1720\n",
      "Epoch 1721, Train loss 3.2629795427481758e-06, Test loss 0.5480454254753983, Train accuracy 100.0, Test accuracy 94.2246835443038, Cost 77.79117012023926 s\n",
      "Epoch 1722, Train loss 3.762681279016655e-06, Test loss 0.5480012389112122, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 77.98763465881348 s\n",
      "Epoch 1723, Train loss 5.90952446103373e-06, Test loss 0.5475968082871618, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 77.97847986221313 s\n",
      "Epoch 1724, Train loss 4.4798335586206505e-06, Test loss 0.5479502117709268, Train accuracy 100.0, Test accuracy 94.10601265822785, Cost 78.0985860824585 s\n",
      "Epoch 1725, Train loss 5.67192033225251e-06, Test loss 0.5480007367986667, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 77.89600467681885 s\n",
      "Model saved in epoch 1725\n",
      "Epoch 1726, Train loss 2.382915346954647e-06, Test loss 0.5482504958216148, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 77.97386240959167 s\n",
      "Epoch 1727, Train loss 2.833355114503486e-06, Test loss 0.5484119072745118, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 77.81874203681946 s\n",
      "Epoch 1728, Train loss 4.405977169005864e-06, Test loss 0.5483096240064765, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 77.94545817375183 s\n",
      "Epoch 1729, Train loss 5.983718247325326e-06, Test loss 0.5462058776918846, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 77.78696942329407 s\n",
      "Epoch 1730, Train loss 8.004851317081837e-06, Test loss 0.5461135981015012, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 77.99530792236328 s\n",
      "Model saved in epoch 1730\n",
      "Epoch 1731, Train loss 2.262881231694711e-06, Test loss 0.5462257977150664, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 77.91531324386597 s\n",
      "Epoch 1732, Train loss 9.314111897011087e-06, Test loss 0.5473890246092519, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 78.00730013847351 s\n",
      "Epoch 1733, Train loss 1.6287321351438236e-06, Test loss 0.5473079935093469, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 77.7729377746582 s\n",
      "Epoch 1734, Train loss 2.7925138373627957e-06, Test loss 0.5473150971192348, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 78.0978455543518 s\n",
      "Epoch 1735, Train loss 3.6207199812241155e-06, Test loss 0.5477676906540424, Train accuracy 100.0, Test accuracy 94.11590189873418, Cost 77.85496640205383 s\n",
      "Model saved in epoch 1735\n",
      "Epoch 1736, Train loss 3.706272924723908e-06, Test loss 0.5478168791042098, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 78.047616481781 s\n",
      "Epoch 1737, Train loss 5.041417538716335e-06, Test loss 0.546605886254884, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 77.83858633041382 s\n",
      "Epoch 1738, Train loss 4.1835082600185015e-06, Test loss 0.5457245907451533, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 78.03298449516296 s\n",
      "Epoch 1739, Train loss 2.4197960538243317e-06, Test loss 0.5457226617992679, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 77.83910250663757 s\n",
      "Epoch 1740, Train loss 1.1889464291763541e-05, Test loss 0.543788664231572, Train accuracy 100.0, Test accuracy 94.21479430379746, Cost 78.08379793167114 s\n",
      "Model saved in epoch 1740\n",
      "Epoch 1741, Train loss 2.0692198004960444e-06, Test loss 0.5435028981558884, Train accuracy 100.0, Test accuracy 94.2246835443038, Cost 78.05190348625183 s\n",
      "Epoch 1742, Train loss 2.3656043146542097e-06, Test loss 0.5431876099562343, Train accuracy 100.0, Test accuracy 94.21479430379746, Cost 78.09322690963745 s\n",
      "Epoch 1743, Train loss 2.0313542652997694e-06, Test loss 0.5436205464073375, Train accuracy 100.0, Test accuracy 94.19501582278481, Cost 77.91145706176758 s\n",
      "Epoch 1744, Train loss 5.403697168199998e-06, Test loss 0.543532907302621, Train accuracy 100.0, Test accuracy 94.23457278481013, Cost 78.13072633743286 s\n",
      "Epoch 1745, Train loss 3.8044508075873154e-06, Test loss 0.5445960870460619, Train accuracy 100.0, Test accuracy 94.20490506329114, Cost 78.04521989822388 s\n",
      "Model saved in epoch 1745\n",
      "Epoch 1746, Train loss 2.413181083774774e-06, Test loss 0.5446134389767164, Train accuracy 100.0, Test accuracy 94.21479430379746, Cost 78.08453154563904 s\n",
      "Epoch 1747, Train loss 2.465125436590302e-06, Test loss 0.5443426305546036, Train accuracy 100.0, Test accuracy 94.24446202531645, Cost 77.63469457626343 s\n",
      "Epoch 1748, Train loss 2.88911279191401e-06, Test loss 0.5443529678107817, Train accuracy 100.0, Test accuracy 94.24446202531645, Cost 77.97340416908264 s\n",
      "Epoch 1749, Train loss 4.920436603205609e-06, Test loss 0.543962290864202, Train accuracy 100.0, Test accuracy 94.23457278481013, Cost 77.80120921134949 s\n",
      "Epoch 1750, Train loss 2.774514216426404e-06, Test loss 0.5438934907883028, Train accuracy 100.0, Test accuracy 94.25435126582279, Cost 78.05164241790771 s\n",
      "Model saved in epoch 1750\n",
      "Epoch 1751, Train loss 2.7983448612165287e-06, Test loss 0.5436877207476881, Train accuracy 100.0, Test accuracy 94.26424050632912, Cost 77.77755045890808 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1752, Train loss 1.2758368791170845e-06, Test loss 0.5436997204264508, Train accuracy 100.0, Test accuracy 94.24446202531645, Cost 78.07162857055664 s\n",
      "Epoch 1753, Train loss 4.337688004574066e-06, Test loss 0.5434280301951155, Train accuracy 100.0, Test accuracy 94.2246835443038, Cost 77.88445854187012 s\n",
      "Epoch 1754, Train loss 3.6460937910300173e-06, Test loss 0.5437971920340876, Train accuracy 100.0, Test accuracy 94.24446202531645, Cost 78.25361728668213 s\n",
      "Epoch 1755, Train loss 6.5946160247479835e-06, Test loss 0.541373585409756, Train accuracy 100.0, Test accuracy 94.21479430379746, Cost 77.86107921600342 s\n",
      "Model saved in epoch 1755\n",
      "Epoch 1756, Train loss 6.887541550432411e-06, Test loss 0.5400297931289371, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 78.0264413356781 s\n",
      "Epoch 1757, Train loss 6.155738196510471e-06, Test loss 0.540657399858855, Train accuracy 100.0, Test accuracy 94.2246835443038, Cost 77.95964884757996 s\n",
      "Epoch 1758, Train loss 1.1109459795588698e-05, Test loss 0.5407243712416178, Train accuracy 100.0, Test accuracy 94.21479430379746, Cost 77.98082494735718 s\n",
      "Epoch 1759, Train loss 7.837000379792414e-06, Test loss 0.542500972276247, Train accuracy 100.0, Test accuracy 94.24446202531645, Cost 77.98890972137451 s\n",
      "Epoch 1760, Train loss 1.316248840829583e-06, Test loss 0.5425345755264729, Train accuracy 100.0, Test accuracy 94.26424050632912, Cost 77.79358458518982 s\n",
      "Model saved in epoch 1760\n",
      "Epoch 1761, Train loss 3.3636790150503996e-06, Test loss 0.542349041640004, Train accuracy 100.0, Test accuracy 94.24446202531645, Cost 77.98308992385864 s\n",
      "Epoch 1762, Train loss 2.518224507813395e-06, Test loss 0.5423674281639389, Train accuracy 100.0, Test accuracy 94.24446202531645, Cost 77.88243556022644 s\n",
      "Epoch 1763, Train loss 1.9230979902005037e-06, Test loss 0.5424121251400513, Train accuracy 100.0, Test accuracy 94.24446202531645, Cost 78.05082678794861 s\n",
      "Epoch 1764, Train loss 1.7510444204958046e-06, Test loss 0.5422314305471468, Train accuracy 100.0, Test accuracy 94.24446202531645, Cost 77.94620156288147 s\n",
      "Epoch 1765, Train loss 3.115877111829285e-06, Test loss 0.5421086006526705, Train accuracy 100.0, Test accuracy 94.26424050632912, Cost 78.03360366821289 s\n",
      "Model saved in epoch 1765\n",
      "Epoch 1766, Train loss 9.439013170169534e-07, Test loss 0.5421330833548232, Train accuracy 100.0, Test accuracy 94.26424050632912, Cost 77.80396699905396 s\n",
      "Epoch 1767, Train loss 1.2154301266934187e-06, Test loss 0.5421486516730695, Train accuracy 100.0, Test accuracy 94.26424050632912, Cost 77.82227730751038 s\n",
      "Epoch 1768, Train loss 3.2687637128096784e-06, Test loss 0.541420091273664, Train accuracy 100.0, Test accuracy 94.25435126582279, Cost 77.8511061668396 s\n",
      "Epoch 1769, Train loss 1.2675986720514261e-06, Test loss 0.5415476404979259, Train accuracy 100.0, Test accuracy 94.25435126582279, Cost 77.97045230865479 s\n",
      "Epoch 1770, Train loss 1.1751852464081873e-06, Test loss 0.5415868726146372, Train accuracy 100.0, Test accuracy 94.25435126582279, Cost 77.67688012123108 s\n",
      "Model saved in epoch 1770\n",
      "Epoch 1771, Train loss 1.5748367674814576e-06, Test loss 0.5415521136756185, Train accuracy 100.0, Test accuracy 94.24446202531645, Cost 77.97230100631714 s\n",
      "Epoch 1772, Train loss 1.4581818618249074e-06, Test loss 0.5416736798007277, Train accuracy 100.0, Test accuracy 94.2246835443038, Cost 77.77053213119507 s\n",
      "Epoch 1773, Train loss 3.049286116464631e-06, Test loss 0.5414376078715807, Train accuracy 100.0, Test accuracy 94.24446202531645, Cost 78.1055121421814 s\n",
      "Epoch 1774, Train loss 2.211432860112533e-06, Test loss 0.5417812616576122, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 77.83780717849731 s\n",
      "Epoch 1775, Train loss 2.335329290719527e-06, Test loss 0.541768673760227, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 78.11685276031494 s\n",
      "Model saved in epoch 1775\n",
      "Epoch 1776, Train loss 1.2612091997029428e-06, Test loss 0.541672004457516, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 77.84554862976074 s\n",
      "Epoch 1777, Train loss 1.6931439201330968e-06, Test loss 0.5417554639751399, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 78.09124803543091 s\n",
      "Epoch 1778, Train loss 1.881381708809456e-06, Test loss 0.5419551042627685, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 77.83378839492798 s\n",
      "Epoch 1779, Train loss 2.1334566338785495e-06, Test loss 0.5425842668436751, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 78.04147458076477 s\n",
      "Epoch 1780, Train loss 1.2935685501174376e-06, Test loss 0.5428276556202128, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 77.91676020622253 s\n",
      "Model saved in epoch 1780\n",
      "Epoch 1781, Train loss 2.429150997802652e-06, Test loss 0.5432963868296599, Train accuracy 100.0, Test accuracy 94.19501582278481, Cost 78.11537957191467 s\n",
      "Epoch 1782, Train loss 9.167622619000865e-07, Test loss 0.5432736939644511, Train accuracy 100.0, Test accuracy 94.19501582278481, Cost 77.7476806640625 s\n",
      "Epoch 1783, Train loss 1.4264854425509153e-06, Test loss 0.5430942560675778, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 78.18045806884766 s\n",
      "Epoch 1784, Train loss 3.010057623227241e-06, Test loss 0.5443489278041864, Train accuracy 100.0, Test accuracy 94.20490506329114, Cost 77.88560199737549 s\n",
      "Epoch 1785, Train loss 1.2029735246944531e-06, Test loss 0.5441937741788128, Train accuracy 100.0, Test accuracy 94.2246835443038, Cost 78.0859785079956 s\n",
      "Model saved in epoch 1785\n",
      "Epoch 1786, Train loss 6.351270344032674e-06, Test loss 0.5442715624842462, Train accuracy 100.0, Test accuracy 94.2246835443038, Cost 77.9461395740509 s\n",
      "Epoch 1787, Train loss 1.939865653377162e-06, Test loss 0.5450514121146142, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 78.03440380096436 s\n",
      "Epoch 1788, Train loss 1.977814484709126e-06, Test loss 0.5450513034681731, Train accuracy 100.0, Test accuracy 94.19501582278481, Cost 77.84652280807495 s\n",
      "Epoch 1789, Train loss 1.6915890068316847e-06, Test loss 0.5448490994263299, Train accuracy 100.0, Test accuracy 94.20490506329114, Cost 77.97230315208435 s\n",
      "Epoch 1790, Train loss 3.0751250323162814e-06, Test loss 0.5455677434802055, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 77.81102323532104 s\n",
      "Model saved in epoch 1790\n",
      "Epoch 1791, Train loss 1.5025027708268754e-06, Test loss 0.5452488307145578, Train accuracy 100.0, Test accuracy 94.20490506329114, Cost 78.0838372707367 s\n",
      "Epoch 1792, Train loss 1.6652144817846467e-06, Test loss 0.5452191808932945, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 77.95389819145203 s\n",
      "Epoch 1793, Train loss 2.6043096962390693e-06, Test loss 0.5446298209172261, Train accuracy 100.0, Test accuracy 94.20490506329114, Cost 78.0707745552063 s\n",
      "Epoch 1794, Train loss 2.0824104077622046e-06, Test loss 0.5442129413161096, Train accuracy 100.0, Test accuracy 94.19501582278481, Cost 78.05916976928711 s\n",
      "Epoch 1795, Train loss 2.0573752389338815e-06, Test loss 0.5441744575583483, Train accuracy 100.0, Test accuracy 94.21479430379746, Cost 78.3478536605835 s\n",
      "Model saved in epoch 1795\n",
      "Epoch 1796, Train loss 7.889928475642545e-07, Test loss 0.5442184810585613, Train accuracy 100.0, Test accuracy 94.21479430379746, Cost 78.13345122337341 s\n",
      "Epoch 1797, Train loss 1.976289217419844e-06, Test loss 0.5441589654435085, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 78.37659072875977 s\n",
      "Epoch 1798, Train loss 2.7693738797459403e-06, Test loss 0.5441187470208241, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 78.1497495174408 s\n",
      "Epoch 1799, Train loss 8.990451447560064e-07, Test loss 0.5441141612356222, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 78.21181559562683 s\n",
      "Epoch 1800, Train loss 1.9951166509133004e-06, Test loss 0.5444434980237032, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 78.32492876052856 s\n",
      "Model saved in epoch 1800\n",
      "Epoch 1801, Train loss 4.7340530626592545e-06, Test loss 0.5455387554402593, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 78.26599264144897 s\n",
      "Epoch 1802, Train loss 1.6824067061483074e-06, Test loss 0.5454790890216827, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 78.34250283241272 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1803, Train loss 1.8425587738018615e-06, Test loss 0.5450450851570202, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 78.33058762550354 s\n",
      "Epoch 1804, Train loss 1.7425251474145584e-06, Test loss 0.5452761610474768, Train accuracy 100.0, Test accuracy 94.1257911392405, Cost 78.18420457839966 s\n",
      "Epoch 1805, Train loss 2.116350095265938e-06, Test loss 0.5451937063208109, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 78.30203747749329 s\n",
      "Model saved in epoch 1805\n",
      "Epoch 1806, Train loss 1.6107758345499142e-06, Test loss 0.5452574397189708, Train accuracy 100.0, Test accuracy 94.1257911392405, Cost 78.23308420181274 s\n",
      "Epoch 1807, Train loss 1.507145346088158e-06, Test loss 0.5452217609633373, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 78.31767797470093 s\n",
      "Epoch 1808, Train loss 1.6032288007436892e-06, Test loss 0.5452018052905421, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 78.0980703830719 s\n",
      "Epoch 1809, Train loss 1.6661747859422383e-06, Test loss 0.5450975325288652, Train accuracy 100.0, Test accuracy 94.11590189873418, Cost 78.11574649810791 s\n",
      "Epoch 1810, Train loss 7.910367822133795e-07, Test loss 0.5450337161368961, Train accuracy 100.0, Test accuracy 94.1257911392405, Cost 77.83360600471497 s\n",
      "Model saved in epoch 1810\n",
      "Epoch 1811, Train loss 1.3359345076890563e-06, Test loss 0.5448854949466789, Train accuracy 100.0, Test accuracy 94.11590189873418, Cost 78.20424771308899 s\n",
      "Epoch 1812, Train loss 2.253961415034768e-06, Test loss 0.5448602207853824, Train accuracy 100.0, Test accuracy 94.11590189873418, Cost 78.12048387527466 s\n",
      "Epoch 1813, Train loss 9.484874856932208e-06, Test loss 0.5484202425879768, Train accuracy 100.0, Test accuracy 94.04667721518987, Cost 78.36703038215637 s\n",
      "Epoch 1814, Train loss 9.787996294722004e-07, Test loss 0.5483738791339005, Train accuracy 100.0, Test accuracy 94.0565664556962, Cost 78.10043811798096 s\n",
      "Epoch 1815, Train loss 4.1246306358563305e-06, Test loss 0.5478961735586577, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.23670387268066 s\n",
      "Model saved in epoch 1815\n",
      "Epoch 1816, Train loss 1.992526824023131e-06, Test loss 0.5478435765150227, Train accuracy 100.0, Test accuracy 94.07634493670886, Cost 78.09075498580933 s\n",
      "Epoch 1817, Train loss 6.940025077329423e-06, Test loss 0.5468133712493921, Train accuracy 100.0, Test accuracy 94.0565664556962, Cost 78.28924441337585 s\n",
      "Epoch 1818, Train loss 1.2094561807729215e-06, Test loss 0.5466864346703396, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 77.98683714866638 s\n",
      "Epoch 1819, Train loss 5.310742056221586e-06, Test loss 0.5452499731054788, Train accuracy 100.0, Test accuracy 94.07634493670886, Cost 78.18158650398254 s\n",
      "Epoch 1820, Train loss 1.146077428824542e-06, Test loss 0.5454586301989193, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 77.99658799171448 s\n",
      "Model saved in epoch 1820\n",
      "Epoch 1821, Train loss 4.590302907204403e-06, Test loss 0.546095366813714, Train accuracy 100.0, Test accuracy 94.07634493670886, Cost 78.44713401794434 s\n",
      "Epoch 1822, Train loss 3.021528540505803e-06, Test loss 0.545868953006177, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.00267338752747 s\n",
      "Epoch 1823, Train loss 8.28698491248164e-06, Test loss 0.5438215293084518, Train accuracy 100.0, Test accuracy 94.0565664556962, Cost 78.29952144622803 s\n",
      "Epoch 1824, Train loss 1.931981602467312e-06, Test loss 0.5436337451580204, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.20777773857117 s\n",
      "Epoch 1825, Train loss 2.9975811466664895e-06, Test loss 0.5437928085462956, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.10436844825745 s\n",
      "Model saved in epoch 1825\n",
      "Epoch 1826, Train loss 2.7620503068981765e-06, Test loss 0.544300353602518, Train accuracy 100.0, Test accuracy 94.03678797468355, Cost 78.43207788467407 s\n",
      "Epoch 1827, Train loss 2.7292770192712407e-06, Test loss 0.5440256122169616, Train accuracy 100.0, Test accuracy 94.01700949367088, Cost 78.50707411766052 s\n",
      "Epoch 1828, Train loss 1.4748433167645606e-06, Test loss 0.5440168527862693, Train accuracy 100.0, Test accuracy 94.01700949367088, Cost 78.35125851631165 s\n",
      "Epoch 1829, Train loss 3.7314940820696567e-06, Test loss 0.5445774832292448, Train accuracy 100.0, Test accuracy 94.02689873417721, Cost 78.59561133384705 s\n",
      "Epoch 1830, Train loss 2.5670824563287016e-06, Test loss 0.5448292479673519, Train accuracy 100.0, Test accuracy 94.04667721518987, Cost 78.30368089675903 s\n",
      "Model saved in epoch 1830\n",
      "Epoch 1831, Train loss 1.099220094350106e-06, Test loss 0.5448655407828621, Train accuracy 100.0, Test accuracy 94.04667721518987, Cost 78.41338777542114 s\n",
      "Epoch 1832, Train loss 1.1490561924941976e-06, Test loss 0.5448601798543448, Train accuracy 100.0, Test accuracy 94.04667721518987, Cost 78.1435968875885 s\n",
      "Epoch 1833, Train loss 1.5626777105513542e-06, Test loss 0.5452444161983985, Train accuracy 100.0, Test accuracy 94.03678797468355, Cost 78.37395858764648 s\n",
      "Epoch 1834, Train loss 1.3936776796728234e-06, Test loss 0.5452560444798651, Train accuracy 100.0, Test accuracy 94.04667721518987, Cost 78.31949615478516 s\n",
      "Epoch 1835, Train loss 1.261024445298783e-06, Test loss 0.5450924660397481, Train accuracy 100.0, Test accuracy 94.03678797468355, Cost 78.28128290176392 s\n",
      "Model saved in epoch 1835\n",
      "Epoch 1836, Train loss 2.692093276509212e-06, Test loss 0.545172203190719, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.21819996833801 s\n",
      "Epoch 1837, Train loss 1.237080191374623e-06, Test loss 0.5451871932873243, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.45889067649841 s\n",
      "Epoch 1838, Train loss 4.195273776557367e-06, Test loss 0.5463717839197267, Train accuracy 100.0, Test accuracy 94.0565664556962, Cost 78.6914427280426 s\n",
      "Epoch 1839, Train loss 1.2503836648848409e-06, Test loss 0.5462300031434132, Train accuracy 100.0, Test accuracy 94.0565664556962, Cost 78.52563500404358 s\n",
      "Epoch 1840, Train loss 1.836236542088717e-06, Test loss 0.5462484948242767, Train accuracy 100.0, Test accuracy 94.04667721518987, Cost 78.17090249061584 s\n",
      "Model saved in epoch 1840\n",
      "Epoch 1841, Train loss 1.3601597956423136e-06, Test loss 0.546346091394183, Train accuracy 100.0, Test accuracy 94.03678797468355, Cost 77.9884283542633 s\n",
      "Epoch 1842, Train loss 1.481623459549795e-06, Test loss 0.5461876834683781, Train accuracy 100.0, Test accuracy 94.03678797468355, Cost 78.78206658363342 s\n",
      "Epoch 1843, Train loss 1.7170381353117312e-06, Test loss 0.5462408172367494, Train accuracy 100.0, Test accuracy 94.03678797468355, Cost 78.374032497406 s\n",
      "Epoch 1844, Train loss 1.7000492676139053e-06, Test loss 0.5462424898449378, Train accuracy 100.0, Test accuracy 94.02689873417721, Cost 78.63158249855042 s\n",
      "Epoch 1845, Train loss 2.208156800863986e-06, Test loss 0.5466313366837139, Train accuracy 100.0, Test accuracy 94.0565664556962, Cost 78.32845163345337 s\n",
      "Model saved in epoch 1845\n",
      "Epoch 1846, Train loss 9.442890971168284e-07, Test loss 0.5466450904554958, Train accuracy 100.0, Test accuracy 94.04667721518987, Cost 78.40071439743042 s\n",
      "Epoch 1847, Train loss 2.4959723282427007e-06, Test loss 0.546705576437938, Train accuracy 100.0, Test accuracy 94.0565664556962, Cost 78.27982068061829 s\n",
      "Epoch 1848, Train loss 1.408347130038681e-06, Test loss 0.5468329784424999, Train accuracy 100.0, Test accuracy 94.07634493670886, Cost 78.52573585510254 s\n",
      "Epoch 1849, Train loss 1.6628213627739316e-06, Test loss 0.5468303947886334, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.02158737182617 s\n",
      "Epoch 1850, Train loss 1.6067562480686894e-06, Test loss 0.5466467304320275, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.49845480918884 s\n",
      "Model saved in epoch 1850\n",
      "Epoch 1851, Train loss 1.2168586601024884e-06, Test loss 0.5466064724929726, Train accuracy 100.0, Test accuracy 94.0565664556962, Cost 78.34025573730469 s\n",
      "Epoch 1852, Train loss 9.720168661497602e-07, Test loss 0.5466083259899405, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.64853024482727 s\n",
      "Epoch 1853, Train loss 1.0474243325564175e-06, Test loss 0.5466551961778086, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.34682106971741 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1854, Train loss 1.124751267787369e-06, Test loss 0.5466563637498059, Train accuracy 100.0, Test accuracy 94.07634493670886, Cost 78.73767638206482 s\n",
      "Epoch 1855, Train loss 6.918895050770757e-06, Test loss 0.547728958292098, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.46906614303589 s\n",
      "Model saved in epoch 1855\n",
      "Epoch 1856, Train loss 1.0323683147011172e-06, Test loss 0.5478288271947752, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.48480033874512 s\n",
      "Epoch 1857, Train loss 1.403638186168438e-06, Test loss 0.5478294999161853, Train accuracy 100.0, Test accuracy 94.0565664556962, Cost 78.47142362594604 s\n",
      "Epoch 1858, Train loss 1.2911547592252097e-06, Test loss 0.5478529068110864, Train accuracy 100.0, Test accuracy 94.04667721518987, Cost 78.5321912765503 s\n",
      "Epoch 1859, Train loss 1.941425158391489e-06, Test loss 0.5478270795149139, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.29547047615051 s\n",
      "Epoch 1860, Train loss 2.041871815102858e-06, Test loss 0.5477465313823917, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.70477962493896 s\n",
      "Model saved in epoch 1860\n",
      "Epoch 1861, Train loss 1.3097976149196137e-06, Test loss 0.5479229860290696, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.23576092720032 s\n",
      "Epoch 1862, Train loss 1.8850075848700646e-06, Test loss 0.5480490966310984, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.79680180549622 s\n",
      "Epoch 1863, Train loss 1.2364847555725094e-06, Test loss 0.5481253421947926, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.43512010574341 s\n",
      "Epoch 1864, Train loss 1.7783784321495192e-06, Test loss 0.5481343604152715, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.57424521446228 s\n",
      "Epoch 1865, Train loss 8.28916351204525e-07, Test loss 0.548149393046204, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.38100337982178 s\n",
      "Model saved in epoch 1865\n",
      "Epoch 1866, Train loss 1.1584915836629158e-06, Test loss 0.5481557964901381, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.48568987846375 s\n",
      "Epoch 1867, Train loss 1.0805144263409607e-06, Test loss 0.5482078896293158, Train accuracy 100.0, Test accuracy 94.0565664556962, Cost 78.43034076690674 s\n",
      "Epoch 1868, Train loss 3.3513628475999084e-06, Test loss 0.5476564284932764, Train accuracy 100.0, Test accuracy 94.07634493670886, Cost 78.81733345985413 s\n",
      "Epoch 1869, Train loss 1.1718045552069226e-06, Test loss 0.5477740580711183, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.1201159954071 s\n",
      "Epoch 1870, Train loss 2.385063714487113e-06, Test loss 0.5475109033758128, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.52635192871094 s\n",
      "Model saved in epoch 1870\n",
      "Epoch 1871, Train loss 1.3256609825856758e-06, Test loss 0.5476171654425089, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.10067820549011 s\n",
      "Epoch 1872, Train loss 1.6900267041373953e-06, Test loss 0.5474606390240826, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.56943607330322 s\n",
      "Epoch 1873, Train loss 1.0450927662147453e-06, Test loss 0.5473336501023437, Train accuracy 100.0, Test accuracy 94.07634493670886, Cost 78.46737933158875 s\n",
      "Epoch 1874, Train loss 2.4171046451795036e-06, Test loss 0.5470640986591955, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.58492040634155 s\n",
      "Epoch 1875, Train loss 5.476795889629559e-06, Test loss 0.5468319809135003, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.31803297996521 s\n",
      "Model saved in epoch 1875\n",
      "Epoch 1876, Train loss 9.67786199367386e-07, Test loss 0.5469216012124773, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.3572039604187 s\n",
      "Epoch 1877, Train loss 1.0942498214822057e-06, Test loss 0.5468631092506119, Train accuracy 100.0, Test accuracy 94.10601265822785, Cost 78.32835698127747 s\n",
      "Epoch 1878, Train loss 1.4830030436473665e-06, Test loss 0.5469542388108712, Train accuracy 100.0, Test accuracy 94.10601265822785, Cost 78.30695414543152 s\n",
      "Epoch 1879, Train loss 1.1189095064900223e-06, Test loss 0.5468864306053028, Train accuracy 100.0, Test accuracy 94.10601265822785, Cost 78.10894012451172 s\n",
      "Epoch 1880, Train loss 1.3234646288398833e-06, Test loss 0.5470551782393758, Train accuracy 100.0, Test accuracy 94.11590189873418, Cost 78.44342303276062 s\n",
      "Model saved in epoch 1880\n",
      "Epoch 1881, Train loss 1.527086650432384e-06, Test loss 0.547089697250837, Train accuracy 100.0, Test accuracy 94.10601265822785, Cost 78.59113645553589 s\n",
      "Epoch 1882, Train loss 1.0191986660977249e-06, Test loss 0.5471886343593839, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.50487780570984 s\n",
      "Epoch 1883, Train loss 1.582455491059165e-06, Test loss 0.5473775661821607, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.35757684707642 s\n",
      "Epoch 1884, Train loss 1.6728552106134848e-06, Test loss 0.5472297898576229, Train accuracy 100.0, Test accuracy 94.10601265822785, Cost 78.35434675216675 s\n",
      "Epoch 1885, Train loss 1.6211079044809903e-06, Test loss 0.5472826787942573, Train accuracy 100.0, Test accuracy 94.11590189873418, Cost 78.48787951469421 s\n",
      "Model saved in epoch 1885\n",
      "Epoch 1886, Train loss 1.3668023374328687e-06, Test loss 0.5471591675960565, Train accuracy 100.0, Test accuracy 94.11590189873418, Cost 78.4321551322937 s\n",
      "Epoch 1887, Train loss 1.2614697011744308e-06, Test loss 0.5471366671255872, Train accuracy 100.0, Test accuracy 94.11590189873418, Cost 77.9936580657959 s\n",
      "Epoch 1888, Train loss 1.7776353876737557e-06, Test loss 0.5467534194452853, Train accuracy 100.0, Test accuracy 94.1257911392405, Cost 78.60707640647888 s\n",
      "Epoch 1889, Train loss 1.8729492013599735e-06, Test loss 0.5468586638003965, Train accuracy 100.0, Test accuracy 94.10601265822785, Cost 78.34650421142578 s\n",
      "Epoch 1890, Train loss 1.0705460713875998e-06, Test loss 0.5468583006081702, Train accuracy 100.0, Test accuracy 94.10601265822785, Cost 78.38631844520569 s\n",
      "Model saved in epoch 1890\n",
      "Epoch 1891, Train loss 1.2605619242545849e-06, Test loss 0.5468812508484985, Train accuracy 100.0, Test accuracy 94.10601265822785, Cost 78.34989786148071 s\n",
      "Epoch 1892, Train loss 1.9956572092560597e-06, Test loss 0.5466990826439254, Train accuracy 100.0, Test accuracy 94.10601265822785, Cost 78.74526691436768 s\n",
      "Epoch 1893, Train loss 4.066026726657046e-06, Test loss 0.5486099427636666, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.25162386894226 s\n",
      "Epoch 1894, Train loss 1.0147628558863944e-06, Test loss 0.5485613990244986, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.3520438671112 s\n",
      "Epoch 1895, Train loss 1.7979685830513885e-06, Test loss 0.5486163599770281, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.21854591369629 s\n",
      "Model saved in epoch 1895\n",
      "Epoch 1896, Train loss 8.205526358988056e-07, Test loss 0.5486357892050019, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.34533452987671 s\n",
      "Epoch 1897, Train loss 9.53824518825096e-07, Test loss 0.5486373790080035, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.36845707893372 s\n",
      "Epoch 1898, Train loss 1.2975793204032332e-06, Test loss 0.548748371061645, Train accuracy 100.0, Test accuracy 94.10601265822785, Cost 78.59978151321411 s\n",
      "Epoch 1899, Train loss 9.217386452164512e-07, Test loss 0.5487494391353824, Train accuracy 100.0, Test accuracy 94.10601265822785, Cost 78.28982090950012 s\n",
      "Epoch 1900, Train loss 1.10405770168429e-06, Test loss 0.5488295871057088, Train accuracy 100.0, Test accuracy 94.10601265822785, Cost 78.81583428382874 s\n",
      "Model saved in epoch 1900\n",
      "Epoch 1901, Train loss 1.8037287367648733e-06, Test loss 0.5490965278276915, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.13635087013245 s\n",
      "Epoch 1902, Train loss 7.97535447446066e-07, Test loss 0.5490570488043979, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.2605836391449 s\n",
      "Epoch 1903, Train loss 7.566887686670176e-07, Test loss 0.5490179790914813, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.41957497596741 s\n",
      "Epoch 1904, Train loss 1.201285241803017e-06, Test loss 0.5489370857801619, Train accuracy 100.0, Test accuracy 94.10601265822785, Cost 78.39795780181885 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1905, Train loss 3.0281689816719094e-06, Test loss 0.548752839995336, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.21201586723328 s\n",
      "Model saved in epoch 1905\n",
      "Epoch 1906, Train loss 1.3149996281387762e-06, Test loss 0.5487113283782066, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.41884636878967 s\n",
      "Epoch 1907, Train loss 1.444863871578482e-06, Test loss 0.5485094859253002, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.30062937736511 s\n",
      "Epoch 1908, Train loss 1.2799328960111746e-06, Test loss 0.5486062378445759, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.61692810058594 s\n",
      "Epoch 1909, Train loss 1.0419955583892139e-06, Test loss 0.5485654550639889, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.17981815338135 s\n",
      "Epoch 1910, Train loss 1.5084620449852725e-06, Test loss 0.5486267457091356, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.90952706336975 s\n",
      "Model saved in epoch 1910\n",
      "Epoch 1911, Train loss 1.6226637618202316e-06, Test loss 0.5483292216741587, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.42883515357971 s\n",
      "Epoch 1912, Train loss 1.736826380858579e-06, Test loss 0.548261551445798, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.59793972969055 s\n",
      "Epoch 1913, Train loss 2.5530422529648903e-06, Test loss 0.5480059989834134, Train accuracy 100.0, Test accuracy 94.10601265822785, Cost 78.2964437007904 s\n",
      "Epoch 1914, Train loss 1.3221052201786546e-06, Test loss 0.5479457772608045, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.5978889465332 s\n",
      "Epoch 1915, Train loss 1.8649727175631185e-06, Test loss 0.5479454057691973, Train accuracy 100.0, Test accuracy 94.07634493670886, Cost 78.38661122322083 s\n",
      "Model saved in epoch 1915\n",
      "Epoch 1916, Train loss 1.1519961700826577e-06, Test loss 0.5480601065143754, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.54457020759583 s\n",
      "Epoch 1917, Train loss 2.3971200867600483e-06, Test loss 0.547956186099143, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.2780237197876 s\n",
      "Epoch 1918, Train loss 6.883456524954396e-07, Test loss 0.5479730296927162, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.41563010215759 s\n",
      "Epoch 1919, Train loss 4.872247981811311e-06, Test loss 0.5472322078068045, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 78.35358572006226 s\n",
      "Epoch 1920, Train loss 1.7414459871069807e-06, Test loss 0.5473207340021676, Train accuracy 100.0, Test accuracy 94.1257911392405, Cost 78.58083891868591 s\n",
      "Model saved in epoch 1920\n",
      "Epoch 1921, Train loss 1.8048311447802734e-06, Test loss 0.547717067637021, Train accuracy 100.0, Test accuracy 94.1257911392405, Cost 78.31988382339478 s\n",
      "Epoch 1922, Train loss 2.1176153968019765e-06, Test loss 0.5477958899698679, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 78.44275045394897 s\n",
      "Epoch 1923, Train loss 1.0422084926405763e-06, Test loss 0.5479132027565679, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 78.12827229499817 s\n",
      "Epoch 1924, Train loss 6.575251773359333e-07, Test loss 0.5479144460604161, Train accuracy 100.0, Test accuracy 94.1257911392405, Cost 78.46966886520386 s\n",
      "Epoch 1925, Train loss 2.167835221763837e-05, Test loss 0.549003592208971, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 78.32133769989014 s\n",
      "Model saved in epoch 1925\n",
      "Epoch 1926, Train loss 1.3809369175146866e-06, Test loss 0.5488914494650273, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 78.30950355529785 s\n",
      "Epoch 1927, Train loss 9.314888378584926e-07, Test loss 0.5488797711043418, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 78.16100811958313 s\n",
      "Epoch 1928, Train loss 1.491805000143226e-06, Test loss 0.548826811151414, Train accuracy 100.0, Test accuracy 94.1257911392405, Cost 78.23384356498718 s\n",
      "Epoch 1929, Train loss 1.7406528570687792e-06, Test loss 0.5489127165343188, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 78.16756343841553 s\n",
      "Epoch 1930, Train loss 2.3647814921873277e-06, Test loss 0.5489485167627093, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 78.53930997848511 s\n",
      "Model saved in epoch 1930\n",
      "Epoch 1931, Train loss 6.725274972083446e-06, Test loss 0.550749423760402, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.1006350517273 s\n",
      "Epoch 1932, Train loss 1.74187681467029e-06, Test loss 0.5508530084279519, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.37120485305786 s\n",
      "Epoch 1933, Train loss 4.2303369410040065e-06, Test loss 0.5499094776526282, Train accuracy 100.0, Test accuracy 94.0565664556962, Cost 78.00800251960754 s\n",
      "Epoch 1934, Train loss 1.3048508272764188e-06, Test loss 0.5498400771919685, Train accuracy 100.0, Test accuracy 94.0565664556962, Cost 78.53141450881958 s\n",
      "Epoch 1935, Train loss 9.61949105473332e-07, Test loss 0.5498426663158815, Train accuracy 100.0, Test accuracy 94.0565664556962, Cost 78.38669538497925 s\n",
      "Model saved in epoch 1935\n",
      "Epoch 1936, Train loss 9.653208191718544e-07, Test loss 0.5497842634780498, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.16331195831299 s\n",
      "Epoch 1937, Train loss 1.4012489752584769e-06, Test loss 0.5496414658389513, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.13006162643433 s\n",
      "Epoch 1938, Train loss 1.2899808060798017e-06, Test loss 0.5495983130192454, Train accuracy 100.0, Test accuracy 94.0565664556962, Cost 78.65466475486755 s\n",
      "Epoch 1939, Train loss 8.756515428832255e-07, Test loss 0.5494902147145211, Train accuracy 100.0, Test accuracy 94.0565664556962, Cost 78.40808176994324 s\n",
      "Epoch 1940, Train loss 1.1444039047257594e-06, Test loss 0.5495472564538822, Train accuracy 100.0, Test accuracy 94.0565664556962, Cost 78.77019309997559 s\n",
      "Model saved in epoch 1940\n",
      "Epoch 1941, Train loss 1.0445680029462426e-06, Test loss 0.549540430778944, Train accuracy 100.0, Test accuracy 94.04667721518987, Cost 78.4809136390686 s\n",
      "Epoch 1942, Train loss 1.8087154742170875e-06, Test loss 0.549617035385174, Train accuracy 100.0, Test accuracy 94.03678797468355, Cost 78.65143752098083 s\n",
      "Epoch 1943, Train loss 9.901717595305283e-07, Test loss 0.549514315267907, Train accuracy 100.0, Test accuracy 94.03678797468355, Cost 78.40093350410461 s\n",
      "Epoch 1944, Train loss 1.6960621999150331e-06, Test loss 0.5494608249090895, Train accuracy 100.0, Test accuracy 94.0565664556962, Cost 78.4975757598877 s\n",
      "Epoch 1945, Train loss 1.3533275455055399e-06, Test loss 0.5496505404574962, Train accuracy 100.0, Test accuracy 94.0565664556962, Cost 78.12319946289062 s\n",
      "Model saved in epoch 1945\n",
      "Epoch 1946, Train loss 1.2162042626466852e-06, Test loss 0.5496154946239689, Train accuracy 100.0, Test accuracy 94.0565664556962, Cost 78.43318104743958 s\n",
      "Epoch 1947, Train loss 1.4713243829456015e-06, Test loss 0.5495684455278553, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.24190378189087 s\n",
      "Epoch 1948, Train loss 1.1897477796437383e-06, Test loss 0.5495859388309189, Train accuracy 100.0, Test accuracy 94.07634493670886, Cost 78.58483123779297 s\n",
      "Epoch 1949, Train loss 2.859356300246466e-06, Test loss 0.5497176562306247, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.30489349365234 s\n",
      "Epoch 1950, Train loss 1.2732857225267563e-06, Test loss 0.5498249627932718, Train accuracy 100.0, Test accuracy 94.0565664556962, Cost 78.733567237854 s\n",
      "Model saved in epoch 1950\n",
      "Epoch 1951, Train loss 2.705044953674178e-06, Test loss 0.5498120737603948, Train accuracy 100.0, Test accuracy 94.07634493670886, Cost 78.14064931869507 s\n",
      "Epoch 1952, Train loss 1.1150743245437754e-06, Test loss 0.5497701532101329, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.49082112312317 s\n",
      "Epoch 1953, Train loss 1.4904436495741524e-06, Test loss 0.5497888131420824, Train accuracy 100.0, Test accuracy 94.07634493670886, Cost 78.27981328964233 s\n",
      "Epoch 1954, Train loss 1.026938757133715e-06, Test loss 0.5497220665970936, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.54104852676392 s\n",
      "Epoch 1955, Train loss 8.854481398466868e-07, Test loss 0.549775635988652, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.20585680007935 s\n",
      "Model saved in epoch 1955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1956, Train loss 1.565587440223821e-06, Test loss 0.5502416993243785, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.55729532241821 s\n",
      "Epoch 1957, Train loss 4.965223140304328e-06, Test loss 0.5497382535964628, Train accuracy 100.0, Test accuracy 94.07634493670886, Cost 78.21040487289429 s\n",
      "Epoch 1958, Train loss 1.3476837766551546e-06, Test loss 0.5497714499884014, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.70480108261108 s\n",
      "Epoch 1959, Train loss 1.1781954831849984e-06, Test loss 0.5497441723754134, Train accuracy 100.0, Test accuracy 94.07634493670886, Cost 78.49419212341309 s\n",
      "Epoch 1960, Train loss 1.0112357309873393e-06, Test loss 0.5496705683726298, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.51669669151306 s\n",
      "Model saved in epoch 1960\n",
      "Epoch 1961, Train loss 8.711927377026529e-07, Test loss 0.5496483715840533, Train accuracy 100.0, Test accuracy 94.10601265822785, Cost 78.31619191169739 s\n",
      "Epoch 1962, Train loss 2.7803516215964703e-06, Test loss 0.5493338917252384, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.60691809654236 s\n",
      "Epoch 1963, Train loss 1.1287861708230978e-06, Test loss 0.5493458409475375, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.40155863761902 s\n",
      "Epoch 1964, Train loss 1.7416904250468897e-06, Test loss 0.548681293248753, Train accuracy 100.0, Test accuracy 94.10601265822785, Cost 78.61255168914795 s\n",
      "Epoch 1965, Train loss 1.0517311057407338e-06, Test loss 0.5486671516223799, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.28620386123657 s\n",
      "Model saved in epoch 1965\n",
      "Epoch 1966, Train loss 1.3425036567212729e-06, Test loss 0.548568609583227, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.38382887840271 s\n",
      "Epoch 1967, Train loss 1.1881972623491865e-06, Test loss 0.5484551967510695, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.3434739112854 s\n",
      "Epoch 1968, Train loss 1.1511448896727544e-06, Test loss 0.5484115693576729, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.78467321395874 s\n",
      "Epoch 1969, Train loss 9.76234766724464e-07, Test loss 0.5483097026148175, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.4326012134552 s\n",
      "Epoch 1970, Train loss 2.6518830772377955e-06, Test loss 0.548208874049066, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.63847398757935 s\n",
      "Model saved in epoch 1970\n",
      "Epoch 1971, Train loss 1.5464106974498015e-06, Test loss 0.5487096130942242, Train accuracy 100.0, Test accuracy 94.07634493670886, Cost 78.3372004032135 s\n",
      "Epoch 1972, Train loss 9.551364371996854e-07, Test loss 0.5487391627098941, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.54137325286865 s\n",
      "Epoch 1973, Train loss 1.209087540276986e-06, Test loss 0.5489658930727953, Train accuracy 100.0, Test accuracy 94.07634493670886, Cost 78.08032321929932 s\n",
      "Epoch 1974, Train loss 7.318224705559053e-07, Test loss 0.5489514650234694, Train accuracy 100.0, Test accuracy 94.07634493670886, Cost 78.49920654296875 s\n",
      "Epoch 1975, Train loss 1.31841805883983e-06, Test loss 0.5495103592453878, Train accuracy 100.0, Test accuracy 94.07634493670886, Cost 78.53753471374512 s\n",
      "Model saved in epoch 1975\n",
      "Epoch 1976, Train loss 8.230130547276973e-07, Test loss 0.5494330357995969, Train accuracy 100.0, Test accuracy 94.07634493670886, Cost 78.74260377883911 s\n",
      "Epoch 1977, Train loss 2.0520118222157413e-06, Test loss 0.5487293976865992, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.32850432395935 s\n",
      "Epoch 1978, Train loss 8.391850326389614e-07, Test loss 0.5487168053094345, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.45947909355164 s\n",
      "Epoch 1979, Train loss 9.358490321828695e-07, Test loss 0.5486913523342036, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.13464069366455 s\n",
      "Epoch 1980, Train loss 1.2059995477770305e-06, Test loss 0.5487647497012645, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.34300351142883 s\n",
      "Model saved in epoch 1980\n",
      "Epoch 1981, Train loss 6.66450324970555e-05, Test loss 0.5568309298044518, Train accuracy 99.99600383631713, Test accuracy 94.09612341772151, Cost 78.09503078460693 s\n",
      "Epoch 1982, Train loss 2.413561517586862e-05, Test loss 0.5506957366685324, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 78.19226813316345 s\n",
      "Epoch 1983, Train loss 6.334358216853307e-06, Test loss 0.5483931307739849, Train accuracy 100.0, Test accuracy 94.19501582278481, Cost 77.88239002227783 s\n",
      "Epoch 1984, Train loss 3.2377260371697804e-06, Test loss 0.5480077787290646, Train accuracy 100.0, Test accuracy 94.20490506329114, Cost 78.1451964378357 s\n",
      "Epoch 1985, Train loss 2.8592323785950373e-06, Test loss 0.5470525143267233, Train accuracy 100.0, Test accuracy 94.19501582278481, Cost 77.86373567581177 s\n",
      "Model saved in epoch 1985\n",
      "Epoch 1986, Train loss 2.1075868114966367e-05, Test loss 0.5457897884181783, Train accuracy 99.99800191815856, Test accuracy 94.2246835443038, Cost 78.01689386367798 s\n",
      "Epoch 1987, Train loss 3.93297037006709e-06, Test loss 0.5469570423040209, Train accuracy 100.0, Test accuracy 94.19501582278481, Cost 77.95623731613159 s\n",
      "Epoch 1988, Train loss 6.363481851398787e-06, Test loss 0.5478105131961122, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 77.98797035217285 s\n",
      "Epoch 1989, Train loss 1.7964515202546052e-06, Test loss 0.5477231390114072, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 77.95572137832642 s\n",
      "Epoch 1990, Train loss 1.3313871193698333e-05, Test loss 0.5471382015679456, Train accuracy 100.0, Test accuracy 94.21479430379746, Cost 77.85702896118164 s\n",
      "Model saved in epoch 1990\n",
      "Epoch 1991, Train loss 9.459164144829113e-06, Test loss 0.5473808807474149, Train accuracy 100.0, Test accuracy 94.20490506329114, Cost 78.0502028465271 s\n",
      "Epoch 1992, Train loss 2.9360185882123715e-06, Test loss 0.5468176218527782, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 77.91305994987488 s\n",
      "Epoch 1993, Train loss 8.609570666717066e-06, Test loss 0.5466128382124479, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 78.08528733253479 s\n",
      "Epoch 1994, Train loss 4.792494887675721e-06, Test loss 0.5474053672220134, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 77.80080914497375 s\n",
      "Epoch 1995, Train loss 1.2384872010467936e-06, Test loss 0.5473485224043266, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 78.0801191329956 s\n",
      "Model saved in epoch 1995\n",
      "Epoch 1996, Train loss 2.268319728915053e-06, Test loss 0.547241640713396, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 77.98569965362549 s\n",
      "Epoch 1997, Train loss 1.4519623215212734e-06, Test loss 0.5473083962179437, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 78.10357069969177 s\n",
      "Epoch 1998, Train loss 2.7986642786972007e-06, Test loss 0.5471572298792344, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 77.96300601959229 s\n",
      "Epoch 1999, Train loss 4.254054290192615e-06, Test loss 0.5472033843398094, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 78.09748339653015 s\n",
      "Epoch 2000, Train loss 2.3826957456704878e-06, Test loss 0.54714400387263, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 78.09374117851257 s\n",
      "Model saved in epoch 2000\n",
      "Epoch 2001, Train loss 2.201274838766322e-06, Test loss 0.5466245938139626, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 78.19345235824585 s\n",
      "Epoch 2002, Train loss 2.703256927571325e-06, Test loss 0.5464871017427384, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 77.75782608985901 s\n",
      "Epoch 2003, Train loss 7.583944911661826e-07, Test loss 0.5464831974498833, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 78.12685656547546 s\n",
      "Epoch 2004, Train loss 1.2217035118957545e-06, Test loss 0.5465746702084059, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 77.76656603813171 s\n",
      "Epoch 2005, Train loss 1.8632057269053194e-06, Test loss 0.5462757634777057, Train accuracy 100.0, Test accuracy 94.20490506329114, Cost 78.09876775741577 s\n",
      "Model saved in epoch 2005\n",
      "Epoch 2006, Train loss 2.5977112851504486e-06, Test loss 0.5461291291668445, Train accuracy 100.0, Test accuracy 94.19501582278481, Cost 77.97576665878296 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2007, Train loss 8.826951857884499e-07, Test loss 0.5461609067418908, Train accuracy 100.0, Test accuracy 94.20490506329114, Cost 78.15830826759338 s\n",
      "Epoch 2008, Train loss 2.4396283460688414e-06, Test loss 0.5462648267798786, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 77.99465417861938 s\n",
      "Epoch 2009, Train loss 1.37744163821184e-06, Test loss 0.5462305008233348, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 78.1947865486145 s\n",
      "Epoch 2010, Train loss 1.8306943656512738e-06, Test loss 0.5462284142835231, Train accuracy 100.0, Test accuracy 94.10601265822785, Cost 78.00477623939514 s\n",
      "Model saved in epoch 2010\n",
      "Epoch 2011, Train loss 1.3198043144978354e-06, Test loss 0.5464053784933272, Train accuracy 100.0, Test accuracy 94.10601265822785, Cost 78.2488694190979 s\n",
      "Epoch 2012, Train loss 1.540697660490407e-06, Test loss 0.5464659701042538, Train accuracy 100.0, Test accuracy 94.10601265822785, Cost 77.88772010803223 s\n",
      "Epoch 2013, Train loss 3.983190031325103e-06, Test loss 0.5473783112402204, Train accuracy 100.0, Test accuracy 94.11590189873418, Cost 78.03065514564514 s\n",
      "Epoch 2014, Train loss 1.4169186326113329e-06, Test loss 0.5473673861426643, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 77.831627368927 s\n",
      "Epoch 2015, Train loss 1.0317335763106816e-06, Test loss 0.5473381419536434, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 78.02205562591553 s\n",
      "Model saved in epoch 2015\n",
      "Epoch 2016, Train loss 1.275298034938515e-06, Test loss 0.5473386889582947, Train accuracy 100.0, Test accuracy 94.11590189873418, Cost 77.9092869758606 s\n",
      "Epoch 2017, Train loss 3.3859895030587405e-06, Test loss 0.5473114147405082, Train accuracy 100.0, Test accuracy 94.19501582278481, Cost 78.13495135307312 s\n",
      "Epoch 2018, Train loss 1.6600175008519503e-06, Test loss 0.5469289714399772, Train accuracy 100.0, Test accuracy 94.1257911392405, Cost 77.89994645118713 s\n",
      "Epoch 2019, Train loss 2.2217763658626524e-06, Test loss 0.5470490335852285, Train accuracy 100.0, Test accuracy 94.11590189873418, Cost 78.12035155296326 s\n",
      "Epoch 2020, Train loss 3.4110442734935623e-06, Test loss 0.5473353912747358, Train accuracy 100.0, Test accuracy 94.07634493670886, Cost 77.85844349861145 s\n",
      "Model saved in epoch 2020\n",
      "Epoch 2021, Train loss 2.9728480490713535e-06, Test loss 0.5473964061540894, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.16499042510986 s\n",
      "Epoch 2022, Train loss 2.6624542584577538e-06, Test loss 0.5482670436554318, Train accuracy 100.0, Test accuracy 94.07634493670886, Cost 77.9096896648407 s\n",
      "Epoch 2023, Train loss 1.0485809688055425e-06, Test loss 0.5482131777496277, Train accuracy 100.0, Test accuracy 94.07634493670886, Cost 78.20610666275024 s\n",
      "Epoch 2024, Train loss 2.147391025717715e-06, Test loss 0.5483497231255604, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 77.86198735237122 s\n",
      "Epoch 2025, Train loss 1.1532616035083646e-06, Test loss 0.5484011824938315, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.14463567733765 s\n",
      "Model saved in epoch 2025\n",
      "Epoch 2026, Train loss 9.872064272229055e-06, Test loss 0.5506763774194295, Train accuracy 100.0, Test accuracy 94.01700949367088, Cost 78.12905263900757 s\n",
      "Epoch 2027, Train loss 1.4964934388668689e-06, Test loss 0.5508162706524511, Train accuracy 100.0, Test accuracy 94.03678797468355, Cost 78.29297399520874 s\n",
      "Epoch 2028, Train loss 1.132147132434846e-06, Test loss 0.550739727725711, Train accuracy 100.0, Test accuracy 94.03678797468355, Cost 77.82549118995667 s\n",
      "Epoch 2029, Train loss 2.1071461522329546e-06, Test loss 0.5495207673763927, Train accuracy 100.0, Test accuracy 94.04667721518987, Cost 78.20287752151489 s\n",
      "Epoch 2030, Train loss 7.893502839787648e-07, Test loss 0.5495336064243619, Train accuracy 100.0, Test accuracy 94.04667721518987, Cost 77.81040120124817 s\n",
      "Model saved in epoch 2030\n",
      "Epoch 2031, Train loss 1.960745236471338e-06, Test loss 0.5494453053497061, Train accuracy 100.0, Test accuracy 94.0565664556962, Cost 78.07717514038086 s\n",
      "Epoch 2032, Train loss 9.58234299688667e-07, Test loss 0.5494717684916303, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 77.78674268722534 s\n",
      "Epoch 2033, Train loss 4.6473461864030666e-06, Test loss 0.5488512115199354, Train accuracy 100.0, Test accuracy 94.01700949367088, Cost 77.9909234046936 s\n",
      "Epoch 2034, Train loss 1.6263372481252313e-06, Test loss 0.5490023435293874, Train accuracy 100.0, Test accuracy 94.03678797468355, Cost 78.05736708641052 s\n",
      "Epoch 2035, Train loss 6.202922232287604e-07, Test loss 0.5490390672525273, Train accuracy 100.0, Test accuracy 94.03678797468355, Cost 78.05617141723633 s\n",
      "Model saved in epoch 2035\n",
      "Epoch 2036, Train loss 2.5370486331692005e-06, Test loss 0.5490048922881295, Train accuracy 100.0, Test accuracy 94.01700949367088, Cost 77.86629176139832 s\n",
      "Epoch 2037, Train loss 8.626837805418744e-07, Test loss 0.5489698743707017, Train accuracy 100.0, Test accuracy 94.00712025316456, Cost 78.0674798488617 s\n",
      "Epoch 2038, Train loss 6.793768193320798e-07, Test loss 0.5489607048751433, Train accuracy 100.0, Test accuracy 94.01700949367088, Cost 77.86254501342773 s\n",
      "Epoch 2039, Train loss 5.874007230563606e-07, Test loss 0.548965482111973, Train accuracy 100.0, Test accuracy 94.01700949367088, Cost 78.13500142097473 s\n",
      "Epoch 2040, Train loss 1.436013034434301e-06, Test loss 0.5489974396326874, Train accuracy 100.0, Test accuracy 94.03678797468355, Cost 77.84230947494507 s\n",
      "Model saved in epoch 2040\n",
      "Epoch 2041, Train loss 1.4198788624813158e-06, Test loss 0.5491668572531471, Train accuracy 100.0, Test accuracy 94.02689873417721, Cost 78.06093716621399 s\n",
      "Epoch 2042, Train loss 1.5714154203217691e-06, Test loss 0.5491339352123344, Train accuracy 100.0, Test accuracy 94.01700949367088, Cost 77.75140929222107 s\n",
      "Epoch 2043, Train loss 2.145908795243948e-06, Test loss 0.5490705467100385, Train accuracy 100.0, Test accuracy 94.00712025316456, Cost 78.13673853874207 s\n",
      "Epoch 2044, Train loss 4.4116702787029463e-07, Test loss 0.5490421781057044, Train accuracy 100.0, Test accuracy 94.01700949367088, Cost 77.89422416687012 s\n",
      "Epoch 2045, Train loss 8.190071294448171e-07, Test loss 0.5490424998010262, Train accuracy 100.0, Test accuracy 94.01700949367088, Cost 78.07331871986389 s\n",
      "Model saved in epoch 2045\n",
      "Epoch 2046, Train loss 1.570143855073817e-06, Test loss 0.5490929328753978, Train accuracy 100.0, Test accuracy 94.02689873417721, Cost 77.8983941078186 s\n",
      "Epoch 2047, Train loss 1.170534554550848e-06, Test loss 0.5488882637287997, Train accuracy 100.0, Test accuracy 94.04667721518987, Cost 77.98868680000305 s\n",
      "Epoch 2048, Train loss 1.6648769614203324e-06, Test loss 0.5493102256633058, Train accuracy 100.0, Test accuracy 94.04667721518987, Cost 77.86060547828674 s\n",
      "Epoch 2049, Train loss 5.769460666786216e-06, Test loss 0.5485232801565642, Train accuracy 100.0, Test accuracy 94.07634493670886, Cost 78.13123679161072 s\n",
      "Epoch 2050, Train loss 2.5364851614361593e-06, Test loss 0.5483060086829753, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 77.88738965988159 s\n",
      "Model saved in epoch 2050\n",
      "Epoch 2051, Train loss 1.39792171969051e-06, Test loss 0.5484056456745425, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.15516638755798 s\n",
      "Epoch 2052, Train loss 1.4253807872858143e-06, Test loss 0.5483446390002589, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 77.86880326271057 s\n",
      "Epoch 2053, Train loss 1.031451658582247e-06, Test loss 0.5482920018743865, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.17548060417175 s\n",
      "Epoch 2054, Train loss 1.9351014321687984e-06, Test loss 0.5479545497064349, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 77.95659303665161 s\n",
      "Epoch 2055, Train loss 1.1141411945638404e-06, Test loss 0.5478683073498025, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 78.07095003128052 s\n",
      "Model saved in epoch 2055\n",
      "Epoch 2056, Train loss 7.659283649926237e-07, Test loss 0.5478444802987424, Train accuracy 100.0, Test accuracy 94.08623417721519, Cost 77.99050426483154 s\n",
      "Epoch 2057, Train loss 1.7626903552236069e-06, Test loss 0.5472480173729644, Train accuracy 100.0, Test accuracy 94.09612341772151, Cost 78.2128164768219 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2058, Train loss 1.6988232561073233e-06, Test loss 0.5472837768589394, Train accuracy 100.0, Test accuracy 94.1257911392405, Cost 77.95064997673035 s\n",
      "Epoch 2059, Train loss 9.342562931717147e-07, Test loss 0.547283266447013, Train accuracy 100.0, Test accuracy 94.1257911392405, Cost 78.14100885391235 s\n",
      "Epoch 2060, Train loss 1.1473894194513668e-06, Test loss 0.5472265978968596, Train accuracy 100.0, Test accuracy 94.1257911392405, Cost 77.79167413711548 s\n",
      "Model saved in epoch 2060\n",
      "Epoch 2061, Train loss 1.550989643170999e-05, Test loss 0.5462115330598022, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 78.08250522613525 s\n",
      "Epoch 2062, Train loss 9.92286600850356e-07, Test loss 0.5463070197007324, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 77.8727171421051 s\n",
      "Epoch 2063, Train loss 1.6673836627972085e-06, Test loss 0.5461056592720973, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 77.91244626045227 s\n",
      "Epoch 2064, Train loss 1.4794806225375732e-05, Test loss 0.545393169586417, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 77.86761999130249 s\n",
      "Epoch 2065, Train loss 1.3862654519866872e-06, Test loss 0.5450891769385036, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 77.98760771751404 s\n",
      "Model saved in epoch 2065\n",
      "Epoch 2066, Train loss 1.1193930491885156e-06, Test loss 0.5451540713068805, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 77.93623661994934 s\n",
      "Epoch 2067, Train loss 2.2448167222302646e-06, Test loss 0.5449852438855775, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 77.88185977935791 s\n",
      "Epoch 2068, Train loss 9.90746979560562e-07, Test loss 0.5449902950962887, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 78.05266094207764 s\n",
      "Epoch 2069, Train loss 1.5025380529201213e-06, Test loss 0.5450515225529671, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 77.94412422180176 s\n",
      "Epoch 2070, Train loss 1.5646842684539958e-06, Test loss 0.5450182909829707, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 78.02865624427795 s\n",
      "Model saved in epoch 2070\n",
      "Epoch 2071, Train loss 4.928982355842545e-06, Test loss 0.5465579651579072, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 77.90611624717712 s\n",
      "Epoch 2072, Train loss 2.784904623700819e-06, Test loss 0.5461675331373758, Train accuracy 100.0, Test accuracy 94.20490506329114, Cost 78.10916328430176 s\n",
      "Epoch 2073, Train loss 1.677043501841245e-06, Test loss 0.545944101071056, Train accuracy 100.0, Test accuracy 94.19501582278481, Cost 77.65958285331726 s\n",
      "Epoch 2074, Train loss 1.8453742694138758e-06, Test loss 0.54617704686862, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 78.09027767181396 s\n",
      "Epoch 2075, Train loss 1.0780564668541198e-06, Test loss 0.5461161717583861, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 77.81290435791016 s\n",
      "Model saved in epoch 2075\n",
      "Epoch 2076, Train loss 1.8458453784105317e-06, Test loss 0.5462374610802795, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 78.15503883361816 s\n",
      "Epoch 2077, Train loss 8.209641287628853e-07, Test loss 0.5462929030196576, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 78.0081729888916 s\n",
      "Epoch 2078, Train loss 8.902231882729731e-07, Test loss 0.5462879392353794, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 78.11529207229614 s\n",
      "Epoch 2079, Train loss 8.53939734641629e-07, Test loss 0.5462734904847567, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 77.89568972587585 s\n",
      "Epoch 2080, Train loss 9.613102726263802e-07, Test loss 0.5462889448751377, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 78.10881233215332 s\n",
      "Model saved in epoch 2080\n",
      "Epoch 2081, Train loss 1.3409848878490267e-06, Test loss 0.5462066777333429, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 77.83435559272766 s\n",
      "Epoch 2082, Train loss 1.7917481289909036e-06, Test loss 0.5463501643153685, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 78.12163400650024 s\n",
      "Epoch 2083, Train loss 3.7017994333885135e-06, Test loss 0.5467826642944843, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 77.89030432701111 s\n",
      "Epoch 2084, Train loss 1.1125280494050416e-06, Test loss 0.5469027143679087, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 78.11782383918762 s\n",
      "Epoch 2085, Train loss 8.949054814513326e-07, Test loss 0.5469834729085995, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 77.9606704711914 s\n",
      "Model saved in epoch 2085\n",
      "Epoch 2086, Train loss 1.7553037645423512e-06, Test loss 0.5465790325705009, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 78.07170462608337 s\n",
      "Epoch 2087, Train loss 1.2164007590036658e-06, Test loss 0.5466289373138283, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 77.99616551399231 s\n",
      "Epoch 2088, Train loss 2.123238655648323e-06, Test loss 0.5468142189179794, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 78.12447714805603 s\n",
      "Epoch 2089, Train loss 9.721899350090893e-07, Test loss 0.5467727165433425, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 78.04020595550537 s\n",
      "Epoch 2090, Train loss 4.234023106414144e-06, Test loss 0.5472346947917456, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 78.1217691898346 s\n",
      "Model saved in epoch 2090\n",
      "Epoch 2091, Train loss 1.8433705115067636e-06, Test loss 0.5473213110920749, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 77.90572214126587 s\n",
      "Epoch 2092, Train loss 1.2140734729066658e-06, Test loss 0.5473480793305591, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 78.09284973144531 s\n",
      "Epoch 2093, Train loss 1.137150728725909e-06, Test loss 0.547353083480008, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 77.821772813797 s\n",
      "Epoch 2094, Train loss 1.9140430955092484e-06, Test loss 0.5477550465472137, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 78.1391921043396 s\n",
      "Epoch 2095, Train loss 1.0518506062701262e-06, Test loss 0.5478311233882662, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 78.03974986076355 s\n",
      "Model saved in epoch 2095\n",
      "Epoch 2096, Train loss 1.4879581459353566e-06, Test loss 0.5479956218336202, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 78.02128911018372 s\n",
      "Epoch 2097, Train loss 1.1737277011805947e-06, Test loss 0.5480089092367812, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 77.9884467124939 s\n",
      "Epoch 2098, Train loss 9.536422267042262e-07, Test loss 0.547979772845401, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 78.04792857170105 s\n",
      "Epoch 2099, Train loss 6.468756014103576e-07, Test loss 0.5479886379045776, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 77.67244529724121 s\n",
      "Epoch 2100, Train loss 1.437886116935992e-06, Test loss 0.548074002035811, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 78.1920120716095 s\n",
      "Model saved in epoch 2100\n",
      "Epoch 2101, Train loss 3.2438557289871334e-06, Test loss 0.5482537005143829, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 78.02887749671936 s\n",
      "Epoch 2102, Train loss 1.019648951863063e-06, Test loss 0.5482617202627508, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 78.10633182525635 s\n",
      "Epoch 2103, Train loss 3.480973474877158e-06, Test loss 0.5488582091240943, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 77.73165845870972 s\n",
      "Epoch 2104, Train loss 8.598560776599964e-07, Test loss 0.5488587661257273, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 78.00805926322937 s\n",
      "Epoch 2105, Train loss 1.0865328896633001e-06, Test loss 0.5489328993083555, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 77.7897481918335 s\n",
      "Model saved in epoch 2105\n",
      "Epoch 2106, Train loss 1.5187824666148154e-06, Test loss 0.5491258286977116, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 78.09997773170471 s\n",
      "Epoch 2107, Train loss 1.1518314021662399e-06, Test loss 0.5491848105116736, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 78.0118522644043 s\n",
      "Epoch 2108, Train loss 1.5758032278261466e-06, Test loss 0.5493223248403284, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 78.21828150749207 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2109, Train loss 1.0090614557184165e-06, Test loss 0.5493203524170043, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 77.93467283248901 s\n",
      "Epoch 2110, Train loss 8.336099304040218e-07, Test loss 0.5492552897975415, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 78.03288054466248 s\n",
      "Model saved in epoch 2110\n",
      "Epoch 2111, Train loss 1.6130617743488324e-06, Test loss 0.5490193612213377, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 77.85948991775513 s\n",
      "Epoch 2112, Train loss 8.152548710143082e-07, Test loss 0.5489767461637908, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 78.04604196548462 s\n",
      "Epoch 2113, Train loss 7.115928340633828e-07, Test loss 0.5489977885839306, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 77.6912214756012 s\n",
      "Epoch 2114, Train loss 7.690096940976795e-07, Test loss 0.5490369294074517, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 78.05870962142944 s\n",
      "Epoch 2115, Train loss 5.922928661334737e-07, Test loss 0.5490662427265433, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 77.90493202209473 s\n",
      "Model saved in epoch 2115\n",
      "Epoch 2116, Train loss 1.1645984756413836e-06, Test loss 0.5490634932736808, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 78.20246386528015 s\n",
      "Epoch 2117, Train loss 6.691978933421383e-06, Test loss 0.5490721748033657, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 77.97078585624695 s\n",
      "Epoch 2118, Train loss 7.849666363551249e-07, Test loss 0.5491358893392961, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 78.23979759216309 s\n",
      "Epoch 2119, Train loss 7.063886671566482e-07, Test loss 0.5491585908811304, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 78.0505805015564 s\n",
      "Epoch 2120, Train loss 9.826563703617678e-07, Test loss 0.5492224582954298, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 78.21101427078247 s\n",
      "Model saved in epoch 2120\n",
      "Epoch 2121, Train loss 8.518201825157452e-07, Test loss 0.5492059142340587, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 77.86768388748169 s\n",
      "Epoch 2122, Train loss 4.972387205799468e-07, Test loss 0.5491768094369128, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 77.92386198043823 s\n",
      "Epoch 2123, Train loss 1.3102891844868476e-06, Test loss 0.5491548963362658, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 77.66981792449951 s\n",
      "Epoch 2124, Train loss 9.033124281669789e-07, Test loss 0.5491697188419632, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 78.02661991119385 s\n",
      "Epoch 2125, Train loss 1.3031745158494392e-06, Test loss 0.5492202964010118, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 77.66286873817444 s\n",
      "Model saved in epoch 2125\n",
      "Epoch 2126, Train loss 1.221543981565064e-06, Test loss 0.549523160710365, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 78.0630373954773 s\n",
      "Epoch 2127, Train loss 8.524903182319165e-07, Test loss 0.5496163139049011, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 77.85306549072266 s\n",
      "Epoch 2128, Train loss 9.839575887705786e-07, Test loss 0.5496628916339029, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 78.27901482582092 s\n",
      "Epoch 2129, Train loss 8.892727776744994e-07, Test loss 0.5497030727093732, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 77.95348572731018 s\n",
      "Epoch 2130, Train loss 1.0352442414382823e-06, Test loss 0.5496505820487119, Train accuracy 100.0, Test accuracy 94.1257911392405, Cost 78.26396465301514 s\n",
      "Model saved in epoch 2130\n",
      "Epoch 2131, Train loss 1.0233070493611815e-06, Test loss 0.5497825321140168, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 78.07287526130676 s\n",
      "Epoch 2132, Train loss 1.0935710831349235e-06, Test loss 0.5498152118694933, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 78.14325618743896 s\n",
      "Epoch 2133, Train loss 4.207660223847493e-07, Test loss 0.5497977776052076, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 77.78460240364075 s\n",
      "Epoch 2134, Train loss 1.1463055590217131e-06, Test loss 0.549752578705172, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 77.93255543708801 s\n",
      "Epoch 2135, Train loss 1.148398368559771e-06, Test loss 0.5498507584952101, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 77.84458518028259 s\n",
      "Model saved in epoch 2135\n",
      "Epoch 2136, Train loss 1.1929575869620852e-06, Test loss 0.5499366794394541, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 77.97032856941223 s\n",
      "Epoch 2137, Train loss 1.224774375757333e-06, Test loss 0.5499988472725772, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 77.75027990341187 s\n",
      "Epoch 2138, Train loss 9.66535332386916e-07, Test loss 0.5500231702116471, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 78.32072043418884 s\n",
      "Epoch 2139, Train loss 2.034400135915029e-06, Test loss 0.5503357127308846, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 78.43925929069519 s\n",
      "Epoch 2140, Train loss 1.151527270966476e-06, Test loss 0.550355247681654, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 78.42128443717957 s\n",
      "Model saved in epoch 2140\n",
      "Epoch 2141, Train loss 9.389289708619299e-07, Test loss 0.5503305372180818, Train accuracy 100.0, Test accuracy 94.13568037974683, Cost 78.31533098220825 s\n",
      "Epoch 2142, Train loss 1.6113905016393776e-06, Test loss 0.5502679016016707, Train accuracy 100.0, Test accuracy 94.1257911392405, Cost 78.3856565952301 s\n",
      "Epoch 2143, Train loss 9.614588298429228e-07, Test loss 0.550385834672783, Train accuracy 100.0, Test accuracy 94.11590189873418, Cost 78.40432333946228 s\n",
      "Epoch 2144, Train loss 5.366753856673702e-07, Test loss 0.5503662602999543, Train accuracy 100.0, Test accuracy 94.11590189873418, Cost 78.52190017700195 s\n",
      "Epoch 2145, Train loss 7.557314410880573e-07, Test loss 0.5503480777710299, Train accuracy 100.0, Test accuracy 94.11590189873418, Cost 78.5736665725708 s\n",
      "Model saved in epoch 2145\n",
      "Epoch 2146, Train loss 3.6257286906677626e-06, Test loss 0.5495853116617927, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 78.27005362510681 s\n",
      "Epoch 2147, Train loss 5.934693575007168e-07, Test loss 0.549580135583123, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 78.54424047470093 s\n",
      "Epoch 2148, Train loss 1.0473243887453046e-06, Test loss 0.5495517365162885, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 78.44158220291138 s\n",
      "Epoch 2149, Train loss 8.912002230650611e-07, Test loss 0.5495979658599142, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 78.59682941436768 s\n",
      "Epoch 2150, Train loss 8.320797866374272e-07, Test loss 0.5496930470572242, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 78.20531797409058 s\n",
      "Model saved in epoch 2150\n",
      "Epoch 2151, Train loss 1.4237183827898548e-06, Test loss 0.5496399845880798, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 78.75015139579773 s\n",
      "Epoch 2152, Train loss 9.035284267187885e-07, Test loss 0.5495261704619927, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 78.61173677444458 s\n",
      "Epoch 2153, Train loss 1.1408886845412017e-06, Test loss 0.5495736564073381, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 78.67199230194092 s\n",
      "Epoch 2154, Train loss 1.2298467761716361e-06, Test loss 0.5496573644348338, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 78.41282534599304 s\n",
      "Epoch 2155, Train loss 7.211469326944037e-07, Test loss 0.549629465028455, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 78.57209467887878 s\n",
      "Model saved in epoch 2155\n",
      "Epoch 2156, Train loss 7.683217198994669e-07, Test loss 0.5496726451983934, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 78.42509818077087 s\n",
      "Epoch 2157, Train loss 7.132574246780622e-07, Test loss 0.5496914104758939, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 78.43532109260559 s\n",
      "Epoch 2158, Train loss 6.623342990657902e-07, Test loss 0.5496915235549589, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 78.19376468658447 s\n",
      "Epoch 2159, Train loss 9.669709309501033e-07, Test loss 0.5497012102528464, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 78.46139907836914 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2160, Train loss 6.589107047867313e-07, Test loss 0.5497284982400604, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 78.45098185539246 s\n",
      "Model saved in epoch 2160\n",
      "Epoch 2161, Train loss 1.2584826481990458e-06, Test loss 0.5495889178371127, Train accuracy 100.0, Test accuracy 94.19501582278481, Cost 78.44504356384277 s\n",
      "Epoch 2162, Train loss 1.238674157274502e-06, Test loss 0.5496304102147682, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 78.18388772010803 s\n",
      "Epoch 2163, Train loss 8.26052733044365e-07, Test loss 0.5496870060510273, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 78.51726865768433 s\n",
      "Epoch 2164, Train loss 7.201832892450535e-07, Test loss 0.5496597239111043, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 78.53054666519165 s\n",
      "Epoch 2165, Train loss 5.981928808894275e-07, Test loss 0.5496677609184121, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 78.53086233139038 s\n",
      "Model saved in epoch 2165\n",
      "Epoch 2166, Train loss 9.714889905605412e-07, Test loss 0.5496644190029253, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 78.23934817314148 s\n",
      "Epoch 2167, Train loss 8.22108560773538e-07, Test loss 0.5496647969076905, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 78.46833610534668 s\n",
      "Epoch 2168, Train loss 1.0026980857984781e-06, Test loss 0.5497740791002407, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 78.19230580329895 s\n",
      "Epoch 2169, Train loss 8.707747030430507e-07, Test loss 0.5498739138622827, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 78.43126797676086 s\n",
      "Epoch 2170, Train loss 6.037389447683905e-07, Test loss 0.5498768233611614, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 78.24799656867981 s\n",
      "Model saved in epoch 2170\n",
      "Epoch 2171, Train loss 9.712344622722116e-07, Test loss 0.5498260359598112, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 78.51440024375916 s\n",
      "Epoch 2172, Train loss 7.648233136098857e-07, Test loss 0.5498400419195996, Train accuracy 100.0, Test accuracy 94.18512658227849, Cost 78.24758672714233 s\n",
      "Epoch 2173, Train loss 1.1382948645784652e-06, Test loss 0.549938531521755, Train accuracy 100.0, Test accuracy 94.17523734177215, Cost 78.61056447029114 s\n",
      "Epoch 2174, Train loss 7.833783490831746e-07, Test loss 0.5499375295601313, Train accuracy 100.0, Test accuracy 94.19501582278481, Cost 78.34417819976807 s\n",
      "Epoch 2175, Train loss 2.198037593863375e-06, Test loss 0.5499847423803957, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 78.50968050956726 s\n",
      "Model saved in epoch 2175\n",
      "Epoch 2176, Train loss 8.025745245929498e-07, Test loss 0.5500201547070395, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 78.62090969085693 s\n",
      "Epoch 2177, Train loss 8.236718952812799e-07, Test loss 0.5500357661254799, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 78.66932344436646 s\n",
      "Epoch 2178, Train loss 3.388950179075821e-06, Test loss 0.5499871362802349, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 78.63095378875732 s\n",
      "Epoch 2179, Train loss 1.0999855245901832e-06, Test loss 0.5500890746524062, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 78.9996645450592 s\n",
      "Epoch 2180, Train loss 6.167045735001694e-07, Test loss 0.5500708661879166, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 78.52140140533447 s\n",
      "Model saved in epoch 2180\n",
      "Epoch 2181, Train loss 1.2834135264327377e-06, Test loss 0.5501677435410174, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 78.57186603546143 s\n",
      "Epoch 2182, Train loss 1.2192729137798264e-06, Test loss 0.5501379348997828, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 78.39687991142273 s\n",
      "Epoch 2183, Train loss 6.337844997358012e-07, Test loss 0.5500928659605074, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 78.64625072479248 s\n",
      "Epoch 2184, Train loss 7.970359300598166e-07, Test loss 0.5501216239755666, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 78.38274097442627 s\n",
      "Epoch 2185, Train loss 2.1579444436928337e-06, Test loss 0.5502026944032198, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 78.66715288162231 s\n",
      "Model saved in epoch 2185\n",
      "Epoch 2186, Train loss 1.0609611220963882e-06, Test loss 0.5500974764552298, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 78.42580389976501 s\n",
      "Epoch 2187, Train loss 5.963365733150208e-07, Test loss 0.5500854056658624, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 78.65128970146179 s\n",
      "Epoch 2188, Train loss 1.6120583653727197e-06, Test loss 0.5499236576164825, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 78.4011902809143 s\n",
      "Epoch 2189, Train loss 6.001672544423968e-07, Test loss 0.5499479748025725, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 78.58607816696167 s\n",
      "Epoch 2190, Train loss 6.621730088244682e-07, Test loss 0.5499266343592089, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 78.44707131385803 s\n",
      "Model saved in epoch 2190\n",
      "Epoch 2191, Train loss 9.882560805993277e-07, Test loss 0.5500167512063738, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 78.59436988830566 s\n",
      "Epoch 2192, Train loss 8.172050360821037e-07, Test loss 0.5499739673318742, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 78.30577898025513 s\n",
      "Epoch 2193, Train loss 5.670870401999087e-07, Test loss 0.5499384361731855, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 78.39476704597473 s\n",
      "Epoch 2194, Train loss 6.66886314177611e-07, Test loss 0.549939478594291, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 78.0533938407898 s\n",
      "Epoch 2195, Train loss 5.270781989239874e-07, Test loss 0.5499308739281907, Train accuracy 100.0, Test accuracy 94.16534810126582, Cost 78.48798131942749 s\n",
      "Model saved in epoch 2195\n",
      "Epoch 2196, Train loss 9.441333928171333e-07, Test loss 0.5499059353259546, Train accuracy 100.0, Test accuracy 94.1554588607595, Cost 78.38116097450256 s\n",
      "Epoch 2197, Train loss 6.42154256719903e-07, Test loss 0.549925720201263, Train accuracy 100.0, Test accuracy 94.14556962025317, Cost 78.5913188457489 s\n",
      "Epoch 2198, Train loss 1.1899886877881235e-06, Test loss 0.5499563492551635, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.58919596672058 s\n",
      "Epoch 2199, Train loss 6.580242519280438e-07, Test loss 0.5499592465879042, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.75404810905457 s\n",
      "Epoch 2200, Train loss 6.491473502380585e-07, Test loss 0.5499579481120351, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.34758377075195 s\n",
      "Model saved in epoch 2200\n",
      "Epoch 2201, Train loss 1.1086638397368237e-06, Test loss 0.5499772738806809, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.46633768081665 s\n",
      "Epoch 2202, Train loss 6.979960662359432e-07, Test loss 0.5500013040779512, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.29311561584473 s\n",
      "Epoch 2203, Train loss 1.1602894745757891e-06, Test loss 0.5500861255428459, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.84102535247803 s\n",
      "Epoch 2204, Train loss 1.169805078904435e-06, Test loss 0.5500688386868827, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.55742025375366 s\n",
      "Epoch 2205, Train loss 6.949966016089995e-07, Test loss 0.5501471921047077, Train accuracy 100.0, Test accuracy 94.0565664556962, Cost 78.56539821624756 s\n",
      "Model saved in epoch 2205\n",
      "Epoch 2206, Train loss 1.0750960546060118e-06, Test loss 0.5501748795939397, Train accuracy 100.0, Test accuracy 94.07634493670886, Cost 78.17812490463257 s\n",
      "Epoch 2207, Train loss 2.1524968241236935e-06, Test loss 0.5498183368693424, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.59826827049255 s\n",
      "Epoch 2208, Train loss 1.246663760889036e-06, Test loss 0.5498496524140805, Train accuracy 100.0, Test accuracy 94.07634493670886, Cost 78.37647914886475 s\n",
      "Epoch 2209, Train loss 1.5616571671363043e-06, Test loss 0.5498989236694348, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.61877393722534 s\n",
      "Epoch 2210, Train loss 4.698187283117314e-07, Test loss 0.549924634491341, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.50903034210205 s\n",
      "Model saved in epoch 2210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2211, Train loss 1.5930434759797817e-06, Test loss 0.5497106282582765, Train accuracy 100.0, Test accuracy 94.0565664556962, Cost 78.72071862220764 s\n",
      "Epoch 2212, Train loss 8.601732817006661e-07, Test loss 0.5497675216462039, Train accuracy 100.0, Test accuracy 94.06645569620254, Cost 78.49431228637695 s\n",
      "Epoch 2213, Train loss 9.464299956907656e-07, Test loss 0.549738815219342, Train accuracy 100.0, Test accuracy 94.0565664556962, Cost 78.65479350090027 s\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5000 # param\n",
    "epoch_start = 0\n",
    "# path = 'adam_rotate_center_crop1.pt'\n",
    "# path = 'block_3.pt'\n",
    "path = 'batch_128_lr_0.1_no_crop_decay_avg_4.pt'\n",
    "\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "test_accuracy_history = []\n",
    "train_accuracy_history = []\n",
    "\n",
    "Loss = torch.nn.CrossEntropyLoss()\n",
    "lr = 0.1 # param\n",
    "lr_min=0.001\n",
    "# optimizer = torch.optim.SGD(model1.parameters(),lr=lr,momentum=0.9,weight_decay=5e-4) # changable optimizer\n",
    "# optimizer = torch.optim.SGD(model1.parameters(),lr=lr,momentum=0.9) # changable optimizer\n",
    "# optimizer = torch.optim.Adam(model1.parameters(),lr=lr, betas=(0.9,0.999), eps=1e-08, amsgrad=False) # changable optimizer\n",
    "momentum = 0.9\n",
    "nesterov = True\n",
    "optimizer = torch.optim.SGD(model1.parameters(),lr=lr,momentum=momentum,nesterov=nesterov)\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "if os.path.exists(path):\n",
    "  checkpoint = torch.load(path)\n",
    "  print('Read model from checkpoint')\n",
    "  model1.cuda().load_state_dict(checkpoint['model_state_dict'])\n",
    "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "  epoch_start = checkpoint['epoch']\n",
    "  Loss = checkpoint['Loss']\n",
    "  train_loss_history = checkpoint['train_loss_history']\n",
    "  test_loss_history = checkpoint['test_loss_history']\n",
    "  test_accuracy_history = checkpoint['test_accuracy_history']\n",
    "  train_accuracy_history = checkpoint['train_accuracy_history']\n",
    "  print('Restart from epoch',epoch_start)\n",
    "    \n",
    "\n",
    "for epoch in range(epoch_start+1, num_epochs + 1):\n",
    "  timestart = time.time()\n",
    "\n",
    "  train_loss = 0.0\n",
    "  test_loss = 0.0\n",
    "  test_accuracy = 0.0\n",
    "  train_accuracy = 0.0\n",
    "\n",
    "  for i, data in enumerate(trainDataLoader):\n",
    "    images, labels = data\n",
    "    images = images.cuda()\n",
    "    labels = labels.cuda()\n",
    "    optimizer.zero_grad()\n",
    "    predicted_output = model1.cuda()(images)\n",
    "    fit = Loss(predicted_output,labels)\n",
    "    fit.backward()\n",
    "    adjust_learning_rate(optimizer=optimizer,current_epoch=epoch,max_epoch=num_epochs,lr_min=lr_min,lr_max=lr,warmup=True)\n",
    "    optimizer.step()\n",
    "    train_loss += fit.item()\n",
    "    train_accuracy += (torch.eq(torch.max(predicted_output,1)[1],labels).sum()/len(labels)*100).data.cpu().numpy()\n",
    "\n",
    "  for i, data in enumerate(testDataLoader):\n",
    "    with torch.no_grad():\n",
    "      images, labels = data\n",
    "      images = images.cuda()\n",
    "      labels = labels.cuda()\n",
    "      predicted_output = model1.cuda()(images)\n",
    "      fit = Loss(predicted_output,labels)\n",
    "      test_loss += fit.item()\n",
    "      test_accuracy += (torch.eq(torch.max(predicted_output,1)[1],labels).sum()/len(labels)*100).data.cpu().numpy()\n",
    "\n",
    "\n",
    "  train_loss = train_loss/len(trainDataLoader)\n",
    "  test_loss = test_loss/len(testDataLoader)\n",
    "  test_accu = test_accuracy/len(testDataLoader)\n",
    "  train_accu = train_accuracy/len(trainDataLoader)\n",
    "  train_loss_history.append(train_loss)\n",
    "  test_loss_history.append(test_loss)\n",
    "  test_accuracy_history.append(test_accu)\n",
    "  train_accuracy_history.append(train_accu)\n",
    "  print('Epoch %s, Train loss %s, Test loss %s, Train accuracy %s, Test accuracy %s, Cost %s s'%(epoch,\n",
    "                                                                                                   train_loss,test_loss,\n",
    "                                                                                                   train_accu,test_accu,\n",
    "                                                                                                   time.time()-timestart))\n",
    "  \n",
    "  if epoch % 5 == 0 and epoch != 0:\n",
    "    torch.save({'epoch':epoch,\n",
    "          'model_state_dict':model1.cuda().state_dict(),\n",
    "          'optimizer_state_dict':optimizer.state_dict(),\n",
    "          'Loss':Loss,\n",
    "          'train_loss_history':train_loss_history,\n",
    "          'test_loss_history':test_loss_history,\n",
    "          'test_accuracy_history':test_accuracy_history,\n",
    "          'train_accuracy_history':train_accuracy_history},path)\n",
    "    print('Model saved in epoch %s'%(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUy08Iyn7tUl",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 611\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(num_epochs),train_loss_history,'-',linewidth=3,label='Train error')\n",
    "plt.plot(range(num_epochs),test_loss_history,'-',linewidth=3,label='Test error')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(num_epochs),train_accuracy_history,'-',linewidth=3,label='Test accuracy')\n",
    "plt.plot(range(num_epochs),test_accuracy_history,'-',linewidth=3,label='Test accuracy')\n",
    "# plt.plot(range(num_epochs),test_accuracy_history,'-',linewidth=3,label='Test accuracy')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVQJgMts7vcg"
   },
   "outputs": [],
   "source": [
    "print('Accuracy:',sum(test_accuracy_history[-5:])/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LaMUB4p_Ucip"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "“ResNet.ipynb”的副本",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0130588af6254c76a2c8c382288cfcea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4aa5668c8eaa49f88148d499240b6ea5",
      "placeholder": "​",
      "style": "IPY_MODEL_a66b114e4595419f84ac44a676a1ca61",
      "value": " 170499072/? [00:03&lt;00:00, 56047078.45it/s]"
     }
    },
    "1121692d91114d58b3db79e41b0a24c9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d40f4bf0c0e48709cdec5f80069ed2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "321ea7fa0b4d4c9dab9ed5231954e54c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3983cfe844f847899487b2745a203a9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8148f05b38d94ec19351ac920b70acba",
       "IPY_MODEL_af7b660f147d4d1d8796d3419673c9b7",
       "IPY_MODEL_0130588af6254c76a2c8c382288cfcea"
      ],
      "layout": "IPY_MODEL_1121692d91114d58b3db79e41b0a24c9"
     }
    },
    "4aa5668c8eaa49f88148d499240b6ea5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8148f05b38d94ec19351ac920b70acba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_accf52fe298b4716af9d9c351e7326dc",
      "placeholder": "​",
      "style": "IPY_MODEL_fb3e769eb8324af3b77041879c9b54cf",
      "value": ""
     }
    },
    "a66b114e4595419f84ac44a676a1ca61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "accf52fe298b4716af9d9c351e7326dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af7b660f147d4d1d8796d3419673c9b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_321ea7fa0b4d4c9dab9ed5231954e54c",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1d40f4bf0c0e48709cdec5f80069ed2f",
      "value": 170498071
     }
    },
    "fb3e769eb8324af3b77041879c9b54cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
